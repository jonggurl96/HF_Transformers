{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 마스크 언어 모델 Fine-Tuning\n",
    "- [강좌링크](https://wikidocs.net/166833)\n",
    "\n",
    "**domain adaptation**: 분야 특화 데이터에 대해 사전 학습된 언어 모델을 fine-tuning하는 프로세스"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ae0b1b5d7b1788a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLM pretrained model 선택하기\n",
    "\n",
    "BERT 계열 중 다운스트림 성능 손실이 거의 없거나 전혀 없이 훨씬 빠르게 학습될 수 있는 DistilBERT 사용.\n",
    "지식 증류(knownledge distillation)라는 특별한 기술은 큰 teacher 모델이 훨씬 작은 student 모델의 학습을 안내한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec2bd20a610c6c0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT number of parameters: 67M\n",
      "BERT number of parameters: 110M\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"DistilBERT number of parameters: {round(distilbert_num_parameters)}M\")\n",
    "print(f\"BERT number of parameters: 110M\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:38.063252Z",
     "start_time": "2023-08-25T01:39:37.353910900Z"
    }
   },
   "id": "9dafdc7900d15f7c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great deal.\n",
      "This is a great success.\n",
      "This is a great adventure.\n",
      "This is a great idea.\n",
      "This is a great feat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4293/1862350943.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_token_index = torch.where(torch.tensor(inputs[\"input_ids\"] == tokenizer.mask_token_id))[1]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# MASK의 위치를 찾고 해당 logits 추출\n",
    "mask_token_index = torch.where(torch.tensor(inputs[\"input_ids\"] == tokenizer.mask_token_id))[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim = 1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:38.376972600Z",
     "start_time": "2023-08-25T01:39:38.065766100Z"
    }
   },
   "id": "a3b7f0d34011da6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋\n",
    "\n",
    "domain adaptation 과정을 보여주기 위해 감정 분석 모델을 벤치마킹하는데 자주 사용되는 영화 리뷰 모음인 유명한 **Large Movie Review Dataset, (IMDb)**를 사용한다. 이 데이터를 사용해 DistilBERT를 fine-tuning함으로써 언어모델이 사전 학습된 wikipedia의 사실적인 데이터에서 영화 리뷰의 보다 주관적인 요소에 맞게 어휘를 조정할 것으로 기대한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7763fce2f38a942"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.132526700Z",
     "start_time": "2023-08-25T01:39:38.372378Z"
    }
   },
   "id": "47ee0d8e44bd59db"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      ">>> Label: 1\n",
      ">>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      ">>> Label: 1\n",
      ">>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      ">>> Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed = 42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\">>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.140643Z",
     "start_time": "2023-08-25T01:39:40.130377900Z"
    }
   },
   "id": "17dbcb0989c7aa21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "auto-regressive modeling과 masked language modeling의 공통 preprocess는 모든 예제를 통합한 다음 전체 말뭉치를 동일한 크기의 청크로 분할하는 것이다. 이는 개별 예제가 너무 길면 절단되어 유용한 정보가 손실될 수 있기 때문이며 따라서 토크나이저에 `truncation=True` 옵션을 **설정하지 않는다.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5f7d240d5ec27b8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 50000\n    })\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        # i번째의 word_ids를 리스트로 반환\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# 빠른 멀티스레딩을 작동시키기 위해 batched=True\n",
    "# 원본 text와 필요없는 label 칼럼은 제거\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched = True, remove_columns = [\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.151449700Z",
     "start_time": "2023-08-25T01:39:40.140643Z"
    }
   },
   "id": "9425d6500aab3746"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.189724500Z",
     "start_time": "2023-08-25T01:39:40.153461400Z"
    }
   },
   "id": "889ac9870c44744b"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review 0 length: 363\n",
      ">>> Review 1 length: 304\n",
      ">>> Review 2 length: 133\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 학습 집합에서 랜덤 리뷰를 가져와 리뷰당 토큰 수 확인\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\">>> Review {idx} length: {len(sample)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.190724400Z",
     "start_time": "2023-08-25T01:39:40.156710300Z"
    }
   },
   "id": "e978a1d1f687f17e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Concatenated reviews length: 800\n"
     ]
    }
   ],
   "source": [
    "concatenated_exaples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_exaples[\"input_ids\"])\n",
    "print(f\">>> Concatenated reviews length: {total_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.190724400Z",
     "start_time": "2023-08-25T01:39:40.161699900Z"
    }
   },
   "id": "3fcbab331cd9bc05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "전체 길이가 확인되었으니 연결된 리뷰를 `block_size`로 지정된 크기의 청크로 분할한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd4cf2137e08fc9a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 32\n"
     ]
    }
   ],
   "source": [
    "# 학습용이니 chunk size를 작게 지정함\n",
    "chunk_size = 128\n",
    "\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_exaples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\">>> Chunk length: {len(chunk)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.190724400Z",
     "start_time": "2023-08-25T01:39:40.166255200Z"
    }
   },
   "id": "71db5b440545c2a6"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "위에서처럼 마지막 청크는 일반적으로 최대 청크 크기보다 작으며 이를 처리하기 위한 방법은 아래와 같다:\n",
    "- truncation\n",
    "- padding\n",
    "\n",
    "여기서는 truncation을 적용하여 위의 예시 로직들을 단일 함수로 구성한다.\n",
    "\"\"\"\n",
    "def group_texts(examples):\n",
    "    # 모든 텍스트 결합\n",
    "    concatenated_exaples = {\n",
    "        k : sum(examples[k], []) for k in examples.keys()\n",
    "    }\n",
    "    \n",
    "    # 결합된 텍스트들에 대한 길이를 구한다.\n",
    "    total_length = len(concatenated_exaples[list(examples.keys())[0]])\n",
    "    \n",
    "    # chunk_size보다 작은 경우 마지막 청크 삭제\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {\n",
    "        k : [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_exaples.items()\n",
    "    }\n",
    "    \n",
    "    # 새로운 칼럼 레이블 생성\n",
    "    r\"\"\"\n",
    "    마스킹 된 언어 모델링의 목표는 입력 배치에서 무작위로 마스킹된 토큰을 예측하는 것이고 labels 열이 정답 역할을 하기 때문이다.\n",
    "    \"\"\"\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.190724400Z",
     "start_time": "2023-08-25T01:39:40.169645800Z"
    }
   },
   "id": "6197fb26e1294b41"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 61290\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 59899\n    })\n    unsupervised: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 122952\n    })\n})"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched = True, num_proc = 8)\n",
    "lm_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.212742300Z",
     "start_time": "2023-08-25T01:39:40.173788200Z"
    }
   },
   "id": "9714460488a0d7ac"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\n",
      "as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.267205600Z",
     "start_time": "2023-08-25T01:39:40.214744200Z"
    }
   },
   "id": "cd85179cb08bcf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer API를 이용한 DistilBERT Fine-Tuning\n",
    "\n",
    "3장의 시퀀스 분류 모델을 fine-tuning하는 것과 거의 동일하지만 텍스트의 각 배치에서 일부 토큰을 무작위로 마스킹할 수 있는 특수 데이터 수집기가 필요하다는 점이 다르다. 이 작업은 `DataCollatorForLanguageModeling`클래스가 수행하며 토큰화와 마스킹 토큰의 비율인 `mlm_probability` 인수를 전달하기만 하면 된다.\n",
    "\n",
    "BERT에서와 같이 논문에서 일반적으로 사용하는 수치인 15%로 지정하고 수행해본다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570b3eff12d7ed6d"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 무작위 masking 실행\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.268250800Z",
     "start_time": "2023-08-25T01:39:40.219742Z"
    }
   },
   "id": "ea77c31114b1683d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [CLS] i [MASK] i am curious - yellow from my video store [MASK] of [MASK] the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. [MASK]. customs if it ever tried to [MASK] this country, therefore being a fan of films considered collection controversial \" i really had to see this for myself. < br / > < br / > the plot is centered around a [MASK] swedish drama [MASK] named recording who wants to learn everything she caniji life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such\n",
      ">>> as the vietnam war and race [MASK] in the united states [MASK] in between asking politicians and [MASK] denizens of stockholm about their opinions on [MASK], she [MASK] sex with her drama teacher, classmates, and married men. < br / [MASK] < br / > what kills polling about i [MASK] curious - yellow is that 40 years ago, this was [MASK] pornographic. really,xy sex [MASK] nu [MASK] scenes are few and far between [MASK] even then it [MASK] s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nu [MASK] are a major staple [MASK] swedish cinema [MASK] even [MASK]mar bergman,\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "\n",
    "test_samples = [\n",
    "\t\t{k: s[k] for k in s.keys() if k != \"word_ids\"}\n",
    "\t\tfor s in samples\n",
    "]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(test_samples)[\"input_ids\"]:\n",
    "    print(f\">>> {tokenizer.decode(chunk)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.268250800Z",
     "start_time": "2023-08-25T01:39:40.224742100Z"
    }
   },
   "id": "681f567268e9d59e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "무작위 masking의 한 가지 부작용은 학습 및 테스트 집합에 대해 동일한 data collator를 사용하기 때문에 `Trainer`를 사용할 때 평가 메트릭이 결정적이지 않다는 것이다. 이는 아래에 &#129303;Accelerate로 Fine-Tuning할 때 사용자 지정 평가 루프의 유연성을 사용해 임의성을 고정하는 방법을 알아본다.\n",
    "\n",
    "마스킹 된 언어 모델링을 위해 모델 학습 시 사용할 수 있는 한 가지 기법은 개별 토큰뿐만 아니라 전체 단어를 함께 마스킹하는 것이다. 이를 전체 단어 마스킹(whole word masking)이라고 한다. 이를 사용하려면 data collator를 직접 구현해야 한다. data collator는 샘플 목록을 가져와 일괄 처리로 변환하는 함수에 불과하다. 이제 이를 구현한다.\n",
    "\n",
    "이전에 계산된 word ids를 사용해 단어 인덱스와 해당 토큰 사이의 맵을 만든 후 다음 마스킹할 단어를 무작위로 결정하고 입력에 해당 마스크를 적용한다. 레이블은 마스크 단어에 해당하는 레이블을 제외하고 모두 -100이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a768aae92bb3679"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [CLS] i [MASK] i [MASK] curious [MASK] yellow from my video [MASK] because of [MASK] the [MASK] [MASK] surrounded it [MASK] [MASK] was first released in 1967. i also heard that [MASK] [MASK] [MASK] [MASK] seized by u. s. [MASK] [MASK] it ever tried to enter this country, [MASK] [MASK] a [MASK] of [MASK] considered \" controversial \" i really had to see [MASK] for myself. < br / > < br / [MASK] the plot is centered around a young swedish [MASK] [MASK] [MASK] [MASK] who wants to learn everything she can about life [MASK] in particular [MASK] [MASK] to focus her attentions to making some sort of documentary on [MASK] the average swede thought about certain [MASK] issues [MASK]\n",
      ">>> [MASK] the [MASK] war and race [MASK] in the [MASK] states [MASK] [MASK] [MASK] [MASK] politicians [MASK] ordinary denizens of [MASK] [MASK] their [MASK] on politics, she has sex with her [MASK] teacher [MASK] classmates, and married men. < [MASK] / [MASK] < br [MASK] > what [MASK] me about i [MASK] curious - yellow is that [MASK] years ago, this [MASK] considered pornographic. really [MASK] the sex and [MASK] [MASK] scenes are few [MASK] far between [MASK] even [MASK] it's not shot like some cheaply made porno. [MASK] [MASK] countrymen mind find it shocking, in [MASK] sex and nudity are a [MASK] [MASK] [MASK] [MASK] cinema. [MASK] [MASK] [MASK] bergman,\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "\tfor feature in features:\n",
    "\t\tword_ids = feature.pop(\"word_ids\") \n",
    "\t\t\n",
    "\t\t# 단어와 해당 토큰 인덱스 간의 map 생성\n",
    "\t\tmapping = collections.defaultdict(list)\n",
    "\t\tcurrent_word_index = -1\n",
    "\t\tcurrent_word = None\n",
    "\t\t\n",
    "\t\tfor idx, word_id in enumerate(word_ids):\n",
    "\t\t\tif word_id is not None:\n",
    "\t\t\t\tif word_id != current_word:\n",
    "\t\t\t\t\t# 새 단어가 시작된 경우\n",
    "\t\t\t\t\tcurrent_word = word_id\n",
    "\t\t\t\t\tcurrent_word_index += 1\n",
    "\t\t\t\tmapping[current_word_index].append(idx)\n",
    "\t\t\n",
    "\t\tr\"\"\"\n",
    "        feature의 word_id가 [0, 0, 1, 2, 3, 4, 5]인 경우 mapping의 형태\n",
    "        mapping = {\n",
    "        \t단어 인덱스: [토큰 인덱스들]\n",
    "            \"0\": [0, 1],\n",
    "            \"1\": [2],\n",
    "            \"2\": [3],\n",
    "            ...\n",
    "            \"5\": [6]\n",
    "        }\n",
    "        word_id와 mapping되는 token_id를 mapping\n",
    "        \"\"\"\n",
    "\t\t\n",
    "\t\t# =================\n",
    "\t\t# 무작위로 단어 마스킹\n",
    "\t\t# =================\n",
    "\t\t\n",
    "\t\t# 이산 확률 분포 (= 이항 분포)\n",
    "\t\t# n = 1이 나올 확률이 0.2인 동전 던지기를 len(mapping)만큼 수행\n",
    "\t\tmask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "\t\t\n",
    "\t\tinput_ids = feature[\"input_ids\"]\n",
    "\t\tlabels = feature[\"labels\"]\n",
    "\t\tnew_labels = [-100] * len(labels)\n",
    "\t\t\n",
    "\t\tfor word_id in np.where(mask)[0]:\n",
    "\t\t\tword_id = word_id.item()  # [0, 0, 0, 0, 1, ...]의 index\n",
    "\t\t\tfor idx in mapping[word_id]:\n",
    "\t\t\t\tnew_labels[idx] = labels[idx]  # 정답인 label은 따로 저장\n",
    "\t\t\t\tinput_ids[idx] = tokenizer.mask_token_id  # 마스킹\n",
    "\t\t\n",
    "                \t\n",
    "\treturn default_data_collator(features)\n",
    "\n",
    "\n",
    "# test\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "\tprint(f\">>> {tokenizer.decode(chunk)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.268250800Z",
     "start_time": "2023-08-25T01:39:40.232556200Z"
    }
   },
   "id": "e050d337c86af5b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 두 개의 data collator가 있으니 나머지 fine-tuning 단계는 동일하다. 먼저 training dataset을 수천개의 예제만 포함하도록 다운샘플링한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42ce92ce9d1631d7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 1000\n    })\n})"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(train_size / 10)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "\t\ttrain_size = train_size, test_size = test_size, seed = 42\n",
    ")\n",
    "downsampled_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:39:40.269308400Z",
     "start_time": "2023-08-25T01:39:40.236556300Z"
    }
   },
   "id": "4b5eda5195ea75aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TrainingArguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "374c251b90024f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_dc_args = TrainingArguments(\n",
    "\t\toutput_dir = f\"../../models/7.2/{model_name}-finetuned-imdb\",\n",
    "\t\toverwrite_output_dir = True,\n",
    "\t\tevaluation_strategy = \"epoch\",\n",
    "\t\tlearning_rate = 2e-5,\n",
    "\t\tweight_decay = 0.01,\n",
    "\t\tper_device_train_batch_size = batch_size,\n",
    "\t\tper_device_eval_batch_size = batch_size,\n",
    "\t\tfp16 = True,\n",
    "\t\tlogging_steps = logging_steps,\n",
    "\t\toptim = \"adamw_torch\", # warning 방지용\n",
    ")\n",
    "\n",
    "training_wwmdc_args = TrainingArguments(\n",
    "\t\toutput_dir = f\"../../models/7.2/{model_name}-finetuned-imdb\",\n",
    "\t\toverwrite_output_dir = True,\n",
    "\t\tevaluation_strategy = \"epoch\",\n",
    "\t\tlearning_rate = 2e-5,\n",
    "\t\tweight_decay = 0.01,\n",
    "\t\tper_device_train_batch_size = batch_size,\n",
    "\t\tper_device_eval_batch_size = batch_size,\n",
    "\t\tfp16 = True,\n",
    "\t\tlogging_steps = logging_steps,\n",
    "\t\toptim = \"adamw_torch\", # warning 방지용\n",
    "\t\tremove_unused_columns = False # 이 새끼가 자꾸 word_ids 칼럼 지워서 에러났음 default True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:44:03.466933100Z",
     "start_time": "2023-08-25T01:44:03.438933100Z"
    }
   },
   "id": "be652a060ff47749"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "809c613e640cf6e8"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer_data_collator = Trainer(\n",
    "\t\tmodel = model,\n",
    "\t\targs = training_dc_args,\n",
    "\t\ttrain_dataset = downsampled_dataset[\"train\"],\n",
    "\t\teval_dataset = downsampled_dataset[\"test\"],\n",
    "\t\tdata_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer_wwmdc = Trainer(\n",
    "\t\tmodel = model,\n",
    "\t\targs = training_wwmdc_args,\n",
    "\t\ttrain_dataset = downsampled_dataset[\"train\"],\n",
    "\t\teval_dataset = downsampled_dataset[\"test\"],\n",
    "\t\tdata_collator = whole_word_masking_data_collator,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:44:04.697700700Z",
     "start_time": "2023-08-25T01:44:04.673602800Z"
    }
   },
   "id": "526f7e8e2eb1e8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 언어 모델을 위한 Preplexity\n",
    "\n",
    "학습할 레이블이 표기된 말뭉치가 제공되는 text classification 또는 question answering같은 작업과 달리, 언어 모델링에는 명시적 레이블이 없다.\n",
    "\n",
    "좋은 언어 모델은 문법적으로 정확한 문장에 높은 확률을 할당하고 말도 안되는 문장에 낮은 확률을 할당한다. 이 말도 안되는 문장이 어떻게 생겼는지에 대한 더 나은 아이디어를 제공하기 위해 온라인에서 **자동 고침 실패(autocorrect fails)** 전체 데이터셋을 찾을 수 있다. 이 데이터셋은 사람의 전화에 있는 모델이 다소 웃기고 종종 부적절한 문장을 생성한 예시들을 볼 수 있다.\n",
    "\n",
    "테스트셋이 대부분 문법적으로 올바른 문장으로 구성되어 있다고 가정하면 언어 모델의 품질을 측정하는 한 가지 방법은 테스트셋의 모든 문장에서 다음 단어에 할당할 확률을 계산하는 것이다. 높은 확률은 모델이 새로운 예시에 놀라거나(surprised) 당황하지(preplexed) 않았음을 나타내며 언어의 기본 문법 패턴을 학습했음을 나타낸다. 이 preplexity에 대한 다양한 수학적 정의가 있지만 이번에 사용할 것은 cross-entropy loss의 exponential로 정의한다.\n",
    "\n",
    "따라서, `Trainer.evaluate()`함수를 사용해 테스트셋에 대한 cross-entropy loss를 계산한 다음 exponential을 취해 pre-trained model의 복잡도를 계산할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881387621d1a415c"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/16 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity before data collator evaluate: 18.78\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/471 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/16 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity after data collator evaluate: 10.85\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/16 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity before whole word masking data collator evaluate: 2.08\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/471 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/16 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity after whole word masking data collator evaluate: 1.90\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "r\"\"\"\n",
    "Preplexity는 모델이 예측하지 못해 당황스럽다는 뜻이므로 작을수록 좋다.\n",
    "training 후 preplexity가 낮아지는지 확인해본다.\n",
    "\"\"\"\n",
    "\n",
    "# 1. data_collator 사용\n",
    "eval_results = trainer_data_collator.evaluate()\n",
    "print(f\">>> Perplexity before data collator evaluate: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer_data_collator.train()\n",
    "eval_results = trainer_data_collator.evaluate()\n",
    "print(f\">>> Perplexity after data collator evaluate: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "\n",
    "# 2. whole_word_masking_data_collator 사용\n",
    "eval_results = trainer_wwmdc.evaluate()\n",
    "print(f\">>> Perplexity before whole word masking data collator evaluate: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer_wwmdc.train()\n",
    "eval_results = trainer_wwmdc.evaluate()\n",
    "print(f\">>> Perplexity after whole word masking data collator evaluate: {math.exp(eval_results['eval_loss']):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T01:48:37.874243100Z",
     "start_time": "2023-08-25T01:44:06.546517200Z"
    }
   },
   "id": "7259587f6d792277"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## &#129303;Accelerate를 활용한 DistilBERT Fine-Tuning\n",
    "\n",
    "위에서 무작위 마스킹이 적용된다는 것을 확인했다. 따라서 각 학습 실행에서 perplexity 점수에 변동이 있을 것이다. 이 무작위성의 원인을 제거하는 한 가지 방법은 전체 테스트 집합에 대해서 마스킹을 한 번만 적용한 다음 &#129303;Transformers의 기본 data collator를 사용해 평가하는 것이다. 이를 어떻게 적용하는지 보기 위해 `DataCollatorForLanguageModeling`을 처음 접했을 때와 유사하게 개별 batch에 마스킹을 적용하는 간단한 함수를 구현해본다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3908d310ed86a74f"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "\tfeatures = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "\tmasked_inputs = data_collator(features)\n",
    "\t\n",
    "\t# 데이터셋의 각 칼럼에 대해서 새로운 \"masked\" 칼럼을 생성\n",
    "\treturn {f\"masked_{k}\" : v.numpy() for k, v in masked_inputs.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:17:12.521238400Z",
     "start_time": "2023-08-25T02:17:12.513238300Z"
    }
   },
   "id": "28cf86763b3c7616"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 함수를 테스트 집합에 적용하고 마스크되지 않은 열을 삭제해 마스크된 열로 교체한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5844c0d276e9b421"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "if \"word_ids\" in downsampled_dataset.keys():\n",
    "\tdownsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "\t\tinsert_random_mask,\n",
    "\t\tbatched = True,\n",
    "\t\tremove_columns = downsampled_dataset[\"test\"].column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "\t\t{\n",
    "\t\t\t\t\"masked_input_ids\": \"input_ids\",\n",
    "\t\t\t\t\"masked_attention_mask\": \"attention_mask\",\n",
    "\t\t\t\t\"masked_labels\": \"labels\"\n",
    "\t\t}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T03:45:30.901272700Z",
     "start_time": "2023-08-25T03:45:30.879648200Z"
    }
   },
   "id": "9df92dbf5d99179e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이전과 같이 DataLoader를 설정할 수 있지만 평가 집합에 대해 &#129303;Transformers의 `default_data_collator`를 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ffcb21657bf4c7"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(\n",
    "\t\tdownsampled_dataset[\"train\"],\n",
    "\t\tshuffle = True,\n",
    "\t\tbatch_size = batch_size,\n",
    "\t\tcollate_fn = data_collator\n",
    ")\n",
    "eval_data_loader = DataLoader(\n",
    "\t\teval_dataset, batch_size = batch_size, collate_fn = default_data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T03:45:31.902543900Z",
     "start_time": "2023-08-25T03:45:31.900033500Z"
    }
   },
   "id": "c05a812b3eba5756"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Load Pre-Trained Model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# AdamW Optimizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# Accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_data_loader, eval_data_loader = accelerator.prepare(\n",
    "\t\tmodel, optimizer, train_data_loader, eval_data_loader\n",
    ")\n",
    "\n",
    "# Sheduler\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_data_loader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "\t\t\"linear\",\n",
    "\t\toptimizer = optimizer,\n",
    "\t\tnum_warmup_steps = 0,\n",
    "\t\tnum_training_steps = num_training_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T03:45:33.232265800Z",
     "start_time": "2023-08-25T03:45:32.092268800Z"
    }
   },
   "id": "72a039465c23f68b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd265094211e670b"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/471 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6157094ce594f689bfde9b2e4bd4493"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity = 11.307821273803711\n",
      ">>> Epoch 1: Perplexity = 11.092926979064941\n",
      ">>> Epoch 2: Perplexity = 11.092926979064941\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\tmodel.train()\n",
    "\tfor batch in train_data_loader:\n",
    "\t\toutputs = model(**batch)\n",
    "\t\tloss = outputs.loss\n",
    "\t\taccelerator.backward(loss)\n",
    "\t\t\n",
    "\t\toptimizer.step()\n",
    "\t\tlr_scheduler.step()\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tprogress_bar.update(1)\n",
    "\t\n",
    "\tmodel.eval()\n",
    "\tlosses = []\n",
    "\tfor batch in eval_data_loader:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\n",
    "\t\tloss = outputs.loss\n",
    "\t\tlosses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\t\t\n",
    "\tlosses = torch.cat(losses) # Concatenate\n",
    "\tlosses = losses[: len(eval_dataset)]\n",
    "\t\n",
    "\ttry:\n",
    "\t\tperplexity = torch.exp(torch.mean(losses))\n",
    "\texcept OverflowError:\n",
    "\t\tperplexity = float(\"inf\")\n",
    "\t\n",
    "\tprint(f\">>> Epoch {epoch}: Perplexity = {perplexity}\")\n",
    "\t\n",
    "\t# Save Checkpoint\n",
    "\taccelerator.wait_for_everyone()\n",
    "\tunwrapped_model = accelerator.unwrap_model(model)\n",
    "\tunwrapped_model.save_pretrained(\"../../models/7.2/model_accelerated\", save_function = accelerator.save)\n",
    "\tif accelerator.is_main_process:\n",
    "\t\ttokenizer.save_pretrained(\"../../models/7.2/model_accelerated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T03:54:49.653503400Z",
     "start_time": "2023-08-25T03:52:30.850903700Z"
    }
   },
   "id": "223d8f47d5d38d08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Fine-Tuned Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336125581ca6cc0a"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great!\n",
      ">>> this is a great.\n",
      ">>> this is a great film\n",
      ">>> this is a great movie\n",
      ">>> this is a great adventure\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "\t\t\"fill-mask\",\n",
    "\t\tmodel = \"../../models/7.2/model_accelerated\"\n",
    ")\n",
    "\n",
    "text = \"This is a great [MASK]\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "\tprint(f\">>> {pred['sequence']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T03:57:22.754957300Z",
     "start_time": "2023-08-25T03:57:19.649525900Z"
    }
   },
   "id": "a360e676e8c2419e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
