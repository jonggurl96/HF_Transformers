{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3장. 사전학습 모델에 대한 미세조정\n",
    "- [강좌링크](https://wikidocs.net/166800)\n",
    "\n",
    "기존 pretrained model의 fine-tuning 방법\n",
    "- Hub에서 대규모 데이터셋 가져오기\n",
    "- 고급 Trainer API를 사용해 Fine-tuning\n",
    "- Custom Training Loop\n",
    "- &#129303;Accelerate 라이브러리를 활용하여 분산 환경에서 custom training loop를 쉽게 실행"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9cf1d991036834a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 데이터 처리 작업\n",
    "\n",
    "2장에서 공부한 것과 같이, PyTorch에서 단일 batch 기반으로 sequence classifier를 학습하는 방법은 다음과 같다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6deb150181121ae1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 2.10GB\n",
      "Allocated GPU Memory: 0.41GB\n",
      "Reserved GPU Memory: 0.46GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 2.10GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from custom_utils import *\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (AdamW,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          get_scheduler)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "wrapper = CustomObject()\n",
    "wrapper.checkpoint = \"bert-base-uncased\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint)\n",
    "wrapper.sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\"\n",
    "]\n",
    "wrapper.batch = wrapper.tokenizer(wrapper.sequences, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "wrapper.batch[\"labels\"] = torch.tensor([1, 1]) # training을 위한 정답 label\n",
    "\n",
    "wrapper.optimizer = AdamW(wrapper.model.parameters())\n",
    "wrapper.loss = wrapper.model(**wrapper.batch).loss\n",
    "wrapper.loss.backward() # 역전파\n",
    "# wrapper.optimizer.zero_grad(): PyTorch는 기본적으로 RNN지원 위해 gradient를 누적함 따라서 업데이트 전 0으로 바꿔줘야 함.\n",
    "wrapper.optimizer.step() # parameter update\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:08:11.873556400Z",
     "start_time": "2023-08-16T05:08:10.272355200Z"
    }
   },
   "id": "4531a39bb83cbf5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 섹션에서는 William B.Dolan과 Chris Brockett의 논문에서 소개된 MRPC(Microsoft Research Paraphrase Corpus) 데이터셋을 예제로 사용한다.\n",
    "\n",
    "이 데이터셋은 5,801건의 문장 쌍으로 구성되어 있으며 각 문장 쌍의 관계가 의역인지 여부를 판단하는 레이블이 존재함."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5362d35c674eaa13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 허브에서 데이터셋 로딩\n",
    "\n",
    "MRPC 데이터셋은 10개의 데이터셋으로 구성된 GLUE 벤치마크 중 하나이다.\n",
    "> GLUE 벤치마크: 10가지 텍스트 분류 작업을 통해 기계학습 모델의 성능을 측정하기 위한 학술적 벤치마크 데이터 집합\n",
    "\n",
    "&#129303;Datasets 라이브러리는 허브에서 데이터셋을 다운로드하고 캐시할 수 있는 쉬운 명령어를 제공한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "552e481e58fee121"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "wrapper.raw_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:19.187472Z",
     "start_time": "2023-08-16T00:34:17.194080900Z"
    }
   },
   "id": "8364ae1991f325a1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "wrapper.raw_train_dataset = wrapper.raw_datasets[\"train\"]\n",
    "print(wrapper.raw_train_dataset[0])\n",
    "print(wrapper.raw_train_dataset.features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:19.191471200Z",
     "start_time": "2023-08-16T00:34:19.186471700Z"
    }
   },
   "id": "bfc5267d577029ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess Dataset\n",
    "\n",
    "다음과 같이 각 쌍의 모든 첫 번째 문장과 두 번째 문장을 각각 직접 토큰화할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d92fcf11eacc2ce"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "wrapper.checkpoint = \"bert-base-uncased\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)\n",
    "\n",
    "wrapper.tokenized_dataset = wrapper.tokenizer(\n",
    "    wrapper.raw_train_dataset[\"sentence1\"],\n",
    "    wrapper.raw_train_dataset[\"sentence2\"],\n",
    "    padding = True,\n",
    "    truncation = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:19.439586400Z",
     "start_time": "2023-08-16T00:34:19.189471700Z"
    }
   },
   "id": "f7c4a9dd17170cd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 방법은 sentence1, sentence2 쌍의 dataset을 만들지만 데이터가 담겨진 다차원 리스트가 키로 지정된 tokenized_dataset이라는 별도의 Python Dictionary를 반환하는 단점이 있으며, 토큰화하는 동안 전체 데이터셋을 저장할 충분한 공간의 RAM이 있는 경우에만 작동한다.\n",
    "\n",
    "특정 데이터를 `dataset` 객체로 유지하기 위해 `Dataset.map()` 메서드를 사용한다. Python의 `map()`과 같은 lambda 방식으로 작동하며 dataset 객체 요소마다 실행할 메서드를 정의하고 다음과 같이 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2361fb8f5482ab3"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "679cacf8ef4f428fa6e59922ee3d0dfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "768c17cb770a435fa927af1f351360bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "788d7918a3314be29c48db768fbf96ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return wrapper.tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
    "\n",
    "\"\"\"\n",
    "한 번에 모든 데이터셋에 토큰화 기능 적용\n",
    "\"\"\"\n",
    "wrapper.tokenized_datasets = wrapper.raw_datasets.map(tokenize_function, batched = True)\n",
    "print(wrapper.tokenized_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:23.083770500Z",
     "start_time": "2023-08-16T00:34:19.498017600Z"
    }
   },
   "id": "233b68e6cab6de6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "&#129303;Datasets 라이브러리는 데이터셋에 새로운 필드들을 추가하는데 이는 preprocess 함수에서 반환된 사전의 각 키에 해당한다.\n",
    "- input_ids\n",
    "- token_type_ids\n",
    "- attention_mask\n",
    "\n",
    "`num_proc` 매개변수를 전달하여 `map()`으로 전처리 기능을 적용할 때 multi-processing을 사용할 수도 있다.\n",
    "\n",
    "&#129303;Tokenizers 라이브러리는 샘플을 더 빠르게 토큰화하기 위해 이미 다중 스레드를 사용하지만 이 라이브러리에서 지원하는 fast tokenizer를 사용하지 않는 경우라면 속도가 더 빨라질 수 있다.\n",
    "\n",
    "전처리 함수가 `map()`을 적용한 데이터셋의 기존 키들(idx, label) 등에 대한 새로운 값을 반환한 경우 기존 필드(idx, label, sentence1, sentence2 등)들을 변경할 수도 있다.\n",
    "\n",
    "마지막으로 할 일은 전체 요소들을 batch로 분리할 때 가장 긴 요소의 길이로 모든 예제를 채우는 것인데 이를 동적 패딩이라고 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95208a79d25b17e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic Padding\n",
    "\n",
    "샘플들을 함께 모아 지정된 크기의 batch로 구성하는 `collate function`은 `DataLoader`를 build할 때 매개변수로 전달할 수 있다.\n",
    "\n",
    "기본값은 단순히 샘플들을 PyTorch 텐서로 변환하고 결합하는 함수이다.\n",
    "\n",
    "위 코드들은 패딩 작업을 미뤄와서 입력값이 모두 동일한 크기가 아닌데 그 이유는 전체 데이터셋이 아닌 개별 batch에 대해 별도로 padding 작업을 수행하여 과도하게 긴 입력으로 인한 과도한 padding 작업을 방지하기 위함이나 이렇게 하면 학습 속도가 빨라지지만 TPU에서 학습하는 경우 문제가 발생할 수 있다. TPU는 추가적인 padding이 필요한 경우에도 전체 데이터셋이 고정된 형태를 선호한다.\n",
    "\n",
    "batch로 분리하려는 데이터셋의 요소 각각에 대해 정확한 수의 padding을 적용할 수 있는 collate function을 정의해야 하는데 &#129303;Transformers 라이브러리는 `DataCollatorWithPadding`을 통해 이런 기능을 제공한다. 이 함수는 Tokenizer를 입력으로 받아 padding 토큰이 무엇인지와 왼쪽과 오른쪽 중 어느 쪽에 padding을 수행할지를 파악한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76d2c17833f152ed"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "wrapper.data_collator = DataCollatorWithPadding(tokenizer = wrapper.tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:25.789461400Z",
     "start_time": "2023-08-16T00:34:25.785461800Z"
    }
   },
   "id": "41bd334591e5ce57"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[50, 59, 47, 67, 59, 50, 62, 32]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.samples = wrapper.tokenized_datasets[\"train\"][:8]\n",
    "wrapper.samples = {k: v for k, v in wrapper.samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in wrapper.samples[\"input_ids\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:27.871656400Z",
     "start_time": "2023-08-16T00:34:27.825299900Z"
    }
   },
   "id": "23481eef47d74133"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'labels': torch.Size([8])}\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 1.07GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "wrapper.batch = wrapper.data_collator(wrapper.samples)\n",
    "print({k: v.shape for k, v in wrapper.batch.items()})\n",
    "\n",
    "del tokenize_function\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:34:28.744245Z",
     "start_time": "2023-08-16T00:34:28.625223600Z"
    }
   },
   "id": "4b76bee514d505c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Trainer API를 이용한 Fine-tuning\n",
    "\n",
    "&#129303;Transformers 라이브러리는 `Trainer` 클래스를 제공해 fine-tuning을 지원한다.\n",
    "\n",
    "위의 모든 예제를 아래 코드로 요약 후 실행했다고 가정한 후 실습함."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78a3a02a03e851f1"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "886121dc4b9a45df963c224d5d18bc8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b9476cb279a4ec38e165b3ac53d0bcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ab712df644d443db2530a049ecdfc7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrapper.raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "wrapper.checkpoint = \"bert-base-uncased\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return wrapper.tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
    "\n",
    "wrapper.tokenized_datasets = wrapper.raw_datasets.map(tokenize_function, batched =  True)\n",
    "wrapper.data_collator = DataCollatorWithPadding(tokenizer = wrapper.tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:37:07.692996300Z",
     "start_time": "2023-08-16T00:37:05.563374400Z"
    }
   },
   "id": "72375e4bef64a06e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Fine-tuning을 위한 하이퍼 파라미터 `TrainingArguments` 클래스 정의 및 모델(`AutoModelForSequenceClassification`) 정의"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e4fbdef1158e63"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "wrapper.training_arguments = TrainingArguments(\"test-trainer\")\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint, num_labels = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:39:38.611811Z",
     "start_time": "2023-08-16T00:39:37.653170500Z"
    }
   },
   "id": "e83020a9e9f7bb37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위에서 정의한 모델, 하이퍼 파라미터, 데이터 셋과 collate function, tokenizer 등의 개체를 전달하여 `Trainer`를 정의하고 `train()`으로 학습을 진행한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2922e9b4b1657b5c"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1377 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=1377, training_loss=0.34335581184558966, metrics={'train_runtime': 136.4452, 'train_samples_per_second': 80.648, 'train_steps_per_second': 10.092, 'total_flos': 405324636337200.0, 'train_loss': 0.34335581184558966, 'epoch': 3.0})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.trainer = Trainer(\n",
    "    wrapper.model,\n",
    "    wrapper.training_arguments,\n",
    "    train_dataset = wrapper.tokenized_datasets[\"train\"],\n",
    "    eval_dataset = wrapper.tokenized_datasets[\"validation\"],\n",
    "    data_collator = wrapper.data_collator,\n",
    "    tokenizer = wrapper.tokenizer\n",
    ")\n",
    "\n",
    "wrapper.trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:44:58.976776200Z",
     "start_time": "2023-08-16T00:42:42.373653900Z"
    }
   },
   "id": "b5d9b03701bb8f03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "fine-tuning이 시작되고 500단계마다 traing loss가 보고되지만 모델 성능이 좋은지 나쁜지는 알려주지 않는 이유는 아래와 같다.\n",
    "1. 학습 과정에서 평가가 수행되도록 trainer에게 `evaluation_strategy` 매개변수를 \"steps\" 또는 \"epochs\"로 지정하지 않았다.\n",
    "2. 평가 방법 혹은 평가 척도를 정의한 `compute_metrics()` 함수를 지정하지 않았다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "655c9aaa8a0b1a8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "유용한 compute_metrics() 함수를 구현하고 이를 사용하는 방법은 `EvalPrediction` 객체(__predictions__ 필드와 __label_ids__ 필드가 포함된 named tuple)를 필요로 하며 문자열과 실수값을 매핑하는 dictionary를 반환한다. 문자열은 metrics의 이름이고 실수값은 해당 metric에 기반한 평가 결과값이다.\n",
    "\n",
    "예측 결과를 얻으려면 `Trainer.predict()` 명령을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23f28c2c39f0fd97"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/51 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "wrapper.predictions = wrapper.trainer.predict(wrapper.tokenized_datasets[\"validation\"])\n",
    "print(wrapper.predictions.predictions.shape, wrapper.predictions.label_ids.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:51:53.770406800Z",
     "start_time": "2023-08-16T00:51:52.337861800Z"
    }
   },
   "id": "6f5854973dd438ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "408은 예측한 데이터의 개수, 데이터셋의 각 요소에 대한 logit값이다. 이 logit값들을 레이블과 비교할 수 있는 예측 결과로 변환하려면 second axis에 존재하는 항목에서 최대값이 있는 인덱스를 가져와야 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c3755b49b5049a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "wrapper.preds = np.argmax(wrapper.predictions.predictions, axis = -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:55:43.154126900Z",
     "start_time": "2023-08-16T00:55:43.138128700Z"
    }
   },
   "id": "bb74933b90fe28fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 `preds`를 레이블과 비교할 수 있다. `datasets` 라이브러리의 `load_metric()` 함수를 이용해 MRPC 데이터셋과 관련된 평가지포(metric)를 로드하고 `compute_metrics()` 함수를 얻을 수 있다. 각 epoch이 끝날 때 metric을 출력하기 위해 compute_metrics() 함수를 사용해 새 Trainer를 정의하고 학습을 진행하면 학습 손실 외에 각 epoch이 끝날 때 validation loss 및 metrics를 보고한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b82bfefc3261f4d"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jongg/.virtualenvs/HF_Transformers/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1377 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 4.27GB\n",
      "Allocated GPU Memory: 0.02GB\n",
      "Reserved GPU Memory: 0.04GB\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    return metric.compute(predictions = predictions, references = labels)\n",
    "\n",
    "\n",
    "wrapper.training_arguments = TrainingArguments(\"test-trainer-with-metric\", evaluation_strategy = \"epoch\")\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint, num_labels = 2)\n",
    "\n",
    "wrapper.trainer = Trainer(\n",
    "    wrapper.model,\n",
    "    wrapper.training_arguments,\n",
    "    train_dataset = wrapper.tokenized_datasets[\"train\"],\n",
    "    eval_dataset = wrapper.tokenized_datasets[\"validation\"],\n",
    "    data_collator = wrapper.data_collator,\n",
    "    tokenizer = wrapper.tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "wrapper.trainer.train()\n",
    "\n",
    "del compute_metrics\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T01:09:24.038553300Z",
     "start_time": "2023-08-16T01:07:00.443967700Z"
    }
   },
   "id": "dcc0b409bc76ad37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Full Training\n",
    "\n",
    "데이터 처리를 완료했다고 가정 하에 Trainer 클래스 없이 위와 동일한 결과를 얻는 방법"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39d2c581c29f5aa0"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0822658bd774565a40538d699015fba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrapper.raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "wrapper.checkpoint = \"bert-base-uncased\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return wrapper.tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
    "\n",
    "wrapper.tokenized_datasets = wrapper.raw_datasets.map(tokenize_function, batched = True)\n",
    "wrapper.data_collator = DataCollatorWithPadding(tokenizer = wrapper.tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T01:23:04.464697200Z",
     "start_time": "2023-08-16T01:23:02.410410700Z"
    }
   },
   "id": "4c983f0d4c2e7a29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 학습을 위한 준비\n",
    "\n",
    "실제 training loop를 정의하기 전에 몇 가지 객체를 정의해야 한다.\n",
    "1. batch를 반복하는데 사용할 `dataloaders`\n",
    "    - Trainer가 자동으로 수행한 몇 가지 작업을 직접 처리하기 위해 tokenized_datasets에 약간의 후처리 적용 필요\n",
    "      1. 모델이 필요로하지 않는 값이 저장된 columns 제거(sentence1, sentence2 등)\n",
    "      2. column label의 이름을 labels로 변경\n",
    "      3. 파이썬 리스트 대신 PyTorch tensor를 반환하도록 datasets 형식 지정\n",
    "2. Optimizer\n",
    "3. Learning rate scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f44320f2ba9c2c"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 75]), 'token_type_ids': torch.Size([8, 75]), 'attention_mask': torch.Size([8, 75])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongg/.virtualenvs/HF_Transformers/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wrapper.tokenized_datasets = wrapper.tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "wrapper.tokenized_datasets = wrapper.tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "wrapper.tokenized_datasets.set_format(\"torch\")\n",
    "print(wrapper.tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "wrapper.train_dataloader = DataLoader(\n",
    "    wrapper.tokenized_datasets[\"train\"],\n",
    "    shuffle = True,\n",
    "    batch_size = 8,\n",
    "    collate_fn = wrapper.data_collator # batch 내에서의 최대 길이로 padding\n",
    ")\n",
    "\n",
    "wrapper.eval_dataloader = DataLoader(\n",
    "    wrapper.tokenized_datasets[\"validation\"],\n",
    "    batch_size = 8,\n",
    "    collate_fn = wrapper.data_collator\n",
    ")\n",
    "\n",
    "# 데이터 처리에 오류가 있는지 확인하기 위해 batch 검사\n",
    "for batch in wrapper.train_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break\n",
    "\n",
    "# model instantiate\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint, num_labels = 2)\n",
    "\n",
    "\n",
    "# Optimizer AdamW, Adam with weight decay regularization\n",
    "wrapper.optimizer = AdamW(wrapper.model.parameters(), lr = 5e-5)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "\"\"\"\n",
    "default의 경우 최대값 5e-5에서 0까지 선형 감쇠한다.\n",
    "이를 적절하게 정의하려면 우리가 수행할 학습 단계의 횟수를 알아야 하는데 이는 epoch * batch 개수(DataLoader의 길이)의 값과 같다.\n",
    "\"\"\"\n",
    "\n",
    "wrapper.num_epochs = 3\n",
    "wrapper.num_training_steps = wrapper.num_epochs * len(wrapper.train_dataloader)\n",
    "wrapper.lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = wrapper.optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = wrapper.num_training_steps\n",
    ")\n",
    "print(wrapper.num_training_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T01:39:07.126496700Z",
     "start_time": "2023-08-16T01:39:06.265944Z"
    }
   },
   "id": "ee08b6a574729ed5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "GPU로 모델 이동 후 학습\n",
    "\n",
    "학습이 언제 끝날 지 정보를 얻기 위해 tqdm 라이브러리를 사용하여 training steps를 기준으로 progress bar를 출력할 수 있도록 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c6ac28197b07a1"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1377 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "498ae46ab95148389c3683d40a4b4929"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrapper.model.to(\"cuda\")\n",
    "\n",
    "wrapper.progress_bar = tqdm(range(wrapper.num_training_steps))\n",
    "\n",
    "wrapper.model.train()\n",
    "for epoch in range(wrapper.num_epochs):\n",
    "    for batch in wrapper.train_dataloader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = wrapper.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward() # 손실값 역전파\n",
    "        \n",
    "        wrapper.optimizer.step() # gradient 적용\n",
    "        wrapper.lr_scheduler.step()\n",
    "        wrapper.optimizer.zero_grad() # gradient 누적 방지 초기화\n",
    "        wrapper.progress_bar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T01:50:04.340522600Z",
     "start_time": "2023-08-16T01:48:04.503368500Z"
    }
   },
   "id": "62c9adce065ed8dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Loop\n",
    "\n",
    "위에서처럼 &#129303;Datasets 라이브러리에서 제공하는 평가 메트릭을 사용한다.\n",
    "\n",
    "이미 metric.compute() 메서드를 살펴보았지만, `metric.add_batch()` 메서드로 평가 루프를 실행하면서 batch별 평가 metric 결과를 누적할 수 있다.\n",
    "\n",
    "모든 batch를 누적하고 나면 그때 `metric.compute()`로 최종 결과를 얻을 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8f81b48a04841b"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 4.28GB\n",
      "Allocated GPU Memory: 0.43GB\n",
      "Reserved GPU Memory: 0.46GB\n"
     ]
    }
   ],
   "source": [
    "wrapper.metric = load_metric(\"glue\", \"mrpc\")\n",
    "wrapper.model.eval()\n",
    "\n",
    "for batch in wrapper.eval_dataloader:\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = wrapper.model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim = -1)\n",
    "    wrapper.metric.add_batch(predictions = predictions, references = batch[\"labels\"])\n",
    "\n",
    "wrapper.metric.compute()\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T01:57:33.256865600Z",
     "start_time": "2023-08-16T01:57:30.780836200Z"
    }
   },
   "id": "ef16d9ce4356fd86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## &#129303;Accelerate 라이브러리를 사용한 가속화\n",
    "\n",
    "accelerator는 windows에서 안되는걸로 알고 [wsl에 ubuntu, cuda toolkit, cudnn 설치](https://www.notion.so/jonggurl96/WSL-Ubuntu-20-04-CUDA-cudnn-965f97ad8a1f4fd4b5c1b03742b8cd15?pvs=4) 후 사용\n",
    "\n",
    "터미널에서 `accelerate config` 명령어로 다음과 같이 설정함: [default_config.yaml](../assets/default_config.yaml)\n",
    "\n",
    "분산 학습 시작 명령어는 `accelerate launch train.py`와 같이 파이썬파일을 명령어로 실행해야 하고 Notebook에서는 다음과 같다.\n",
    "\n",
    "```python\n",
    "def training_function():\n",
    "    pass\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b2f78451229c1e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training Loop with Accelerate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4be4ff68893b9f7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1ffe5fd6e124ac688a63e4a8a76b7ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72b224ced38145bd9b3be6cf464764ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad0ebf3d0fe542d9a0b50756f83df8e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1377 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4efc1b420e9a4813908881be819339c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    }
   ],
   "source": [
    "wrapper.checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# Tokenizer\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)\n",
    "\n",
    "# Tokenize Function\n",
    "wrapper.raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "def tokenize_function(example):\n",
    "    return wrapper.tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
    "\n",
    "wrapper.tokenized_datasets = wrapper.raw_datasets.map(tokenize_function, batched = True)\n",
    "wrapper.tokenized_datasets = wrapper.tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "wrapper.tokenized_datasets = wrapper.tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "wrapper.tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Collate Function\n",
    "wrapper.data_collator = DataCollatorWithPadding(tokenizer = wrapper.tokenizer)\n",
    "\n",
    "# Training DataLoader\n",
    "wrapper.train_dataloader = DataLoader(\n",
    "    wrapper.tokenized_datasets[\"train\"],\n",
    "    shuffle = True,\n",
    "    batch_size = 8,\n",
    "    collate_fn = wrapper.data_collator\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "wrapper.eval_dataloader = DataLoader(\n",
    "    wrapper.tokenized_datasets[\"validation\"],\n",
    "    batch_size = 8,\n",
    "    collate_fn = wrapper.data_collator\n",
    ")\n",
    "\n",
    "# Model\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint, num_labels = 2)\n",
    "\n",
    "# Optimizer\n",
    "wrapper.optimizer = AdamW(wrapper.model.parameters(), lr = 3e-5)\n",
    "\n",
    "# accelerate\n",
    "wrapper.accelerator = Accelerator()\n",
    "wrapper.train_dataloader, wrapper.eval_dataloader, wrapper.model, wrapper.optimizer = wrapper.accelerator.prepare(\n",
    "    wrapper.train_dataloader, wrapper.eval_dataloader, wrapper.model, wrapper.optimizer\n",
    ")\n",
    "\n",
    "wrapper.num_epochs = 3\n",
    "wrapper.num_training_steps = wrapper.num_epochs * len(wrapper.train_dataloader)\n",
    "wrapper.lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = wrapper.optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = wrapper.num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(wrapper.num_training_steps))\n",
    "\n",
    "def training_fn():\n",
    "    wrapper.model.train()\n",
    "    \n",
    "    for epoch in range(wrapper.num_epochs):\n",
    "        for batch in wrapper.train_dataloader:\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "            outputs = wrapper.model(**batch)\n",
    "            loss = outputs.loss\n",
    "            wrapper.accelerator.backward(loss)\n",
    "            \n",
    "            wrapper.optimizer.step()\n",
    "            wrapper.lr_scheduler.step()\n",
    "            wrapper.optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "\n",
    "notebook_launcher(training_fn, num_processes = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:18:48.152361300Z",
     "start_time": "2023-08-16T05:15:51.721662100Z"
    }
   },
   "id": "e8828b6de4b5ef90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation Loop with Accelerate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6aa6070c8156c82"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "{'accuracy': 0.8627450980392157, 'f1': 0.902439024390244}\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 4.57GB\n",
      "Allocated GPU Memory: 0.02GB\n",
      "Reserved GPU Memory: 0.04GB\n"
     ]
    }
   ],
   "source": [
    "# 평가 메트릭\n",
    "wrapper.metric= load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "def eval_fn():\n",
    "    wrapper.model.eval()\n",
    "    for batch in wrapper.eval_dataloader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = wrapper.model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim = -1)\n",
    "        wrapper.metric.add_batch(predictions = predictions, references = batch[\"labels\"])\n",
    "    \n",
    "    # 평가 결과 계산 및 출력\n",
    "    print(wrapper.metric.compute())\n",
    "    \n",
    "notebook_launcher(eval_fn, num_processes = 1)\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:19:27.362608500Z",
     "start_time": "2023-08-16T05:19:24.991831900Z"
    }
   },
   "id": "55a7730c8bc762fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8e84ad414c4eb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
