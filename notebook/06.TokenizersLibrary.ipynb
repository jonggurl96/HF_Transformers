{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6장. &#129303;Tokenizers 라이브러리\n",
    "- [강좌링크](https://wikidocs.net/166814)\n",
    "\n",
    "> 이 장에서 다룰 주제:\n",
    "> - 새로운 텍스트 말뭉치를 기반으로 특정 체크포인트의 토크나이저와 유사한 새로운 토크나이저를 학습시키는 방법\n",
    "> - fast tokenizer의 특징\n",
    "> - NLP에서 사용되는 세 가지 주요 단어 토큰화 알고리즘의 차이점\n",
    "> - &#129303;Tokenizers 라이브러리를 사용하여 처음부터 토크나이저를 구축하고 특정 데이터로 학습하는 방법\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46082274e221f56e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-22 13:55:37,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from custom_utils import *\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForTokenClassification, AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wrapper = CustomObject()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T04:55:37.526966600Z",
     "start_time": "2023-08-22T04:55:32.953283200Z"
    }
   },
   "id": "28aa63cad74a5677"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 기존 토크나이저에서 새로운 토크나이저 학습\n",
    "각각의 배치에 대해 loss를 조금씩 더 작게 만드는 무작위 확률적 경사 하강법(stochastic gradient descent)으로 학습하는 모델과 달리 토크나이저 학습은 어떤 subword를 선택하는 것이 가장 좋은지 식별하려는 통계적 프로세스이며 결정론적(deterministic)이다.\n",
    "> 동일한 말뭉치에서 동일한 알고리즘으로 학습 시 항상 동일한 결과를 얻을 수 있다.\n",
    "\n",
    "## 말뭉치 모으기\n",
    "&#129303;Transformers에는 기존에 존재하는 것들과 동일한 특성을 가진 새로운 토크나이저 학습 시 사용 가능한 매우 간단한 `AutoTokenizer.train_new_from_iterator()` API가 있다. 이를 실제로 실행하기 위해 GPT-2를 영어가 아닌 다른 언어로 학습한다고 가정한다. CodeSearchNet Challenge를 위해 생성된 CodeSearchNet 데이터셋을 로드한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5234ccc6e0c2bf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a2f09239cb24e2cafb875e33917acea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9156f9b566074f66921ff46c77c38528"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c32a3fc10ad34f06977d11bd2f5e7943"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d9fcaa9df46454d9dccd05069dafb0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3dc8d91239345379fa1676bb0830fad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d719459153694186a321d796f86077fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cbbbf84b25f440ea75ce4aaf2764009"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "643301bd328a4531b6e994e837d16b0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "204cbf8540da4338b43cc58dec15f707"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "003e0846f02a42dd8762c078836b068c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n    num_rows: 412178\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "wrapper.raw_datasets[\"train\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T07:49:08.077865400Z",
     "start_time": "2023-08-21T07:46:59.696923300Z"
    }
   },
   "id": "2d6c93d2129a409f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 데이터셋은 파이썬 코드에서 주석만 분리하고 있다. 토크나이저를 학습하기 위해 `whole_func_string` 칼럼만 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48f795118d2c9ef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'def get_new_token(self, netloc):\\n        \"\"\"Get a new token from BIG-IP and store it internally.\\n\\n        Throws relevant exception if it fails to get a new token.\\n\\n        This method will be called automatically if a request is attempted\\n        but there is no authentication token, or the authentication token\\n        is expired.  It is usually not necessary for users to call it, but\\n        it can be called if it is known that the authentication token has\\n        been invalidated by other means.\\n        \"\"\"\\n        login_body = {\\n            \\'username\\': self.username,\\n            \\'password\\': self.password,\\n        }\\n\\n        if self.auth_provider:\\n            if self.auth_provider == \\'local\\':\\n                login_body[\\'loginProviderName\\'] = \\'local\\'\\n            elif self.auth_provider == \\'tmos\\':\\n                login_body[\\'loginProviderName\\'] = \\'tmos\\'\\n            elif self.auth_provider not in [\\'none\\', \\'default\\']:\\n                providers = self.get_auth_providers(netloc)\\n                for provider in providers:\\n                    if self.auth_provider in provider[\\'link\\']:\\n                        login_body[\\'loginProviderName\\'] = provider[\\'name\\']\\n                        break\\n                    elif self.auth_provider == provider[\\'name\\']:\\n                        login_body[\\'loginProviderName\\'] = provider[\\'name\\']\\n                        break\\n        else:\\n            if self.login_provider_name == \\'tmos\\':\\n                login_body[\\'loginProviderName\\'] = self.login_provider_name\\n\\n        login_url = \"https://%s/mgmt/shared/authn/login\" % (netloc)\\n\\n        response = requests.post(\\n            login_url,\\n            json=login_body,\\n            verify=self.verify,\\n            auth=HTTPBasicAuth(self.username, self.password)\\n        )\\n        self.attempts += 1\\n        if not response.ok or not hasattr(response, \"json\"):\\n            error_message = \\'%s Unexpected Error: %s for uri: %s\\\\nText: %r\\' %\\\\\\n                            (response.status_code,\\n                             response.reason,\\n                             response.url,\\n                             response.text)\\n            raise iControlUnexpectedHTTPError(error_message,\\n                                              response=response)\\n        respJson = response.json()\\n\\n        token = self._get_token_from_response(respJson)\\n        created_bigip = self._get_last_update_micros(token)\\n\\n        try:\\n            expiration_bigip = self._get_expiration_micros(\\n                token, created_bigip\\n            )\\n        except (KeyError, ValueError):\\n            error_message = \\\\\\n                \\'%s Unparseable Response: %s for uri: %s\\\\nText: %r\\' %\\\\\\n                (response.status_code,\\n                 response.reason,\\n                 response.url,\\n                 response.text)\\n            raise iControlUnexpectedHTTPError(error_message,\\n                                              response=response)\\n\\n        try:\\n            self.expiration = self._get_token_expiration_time(\\n                created_bigip, expiration_bigip\\n            )\\n        except iControlUnexpectedHTTPError:\\n            error_message = \\\\\\n                \\'%s Token already expired: %s for uri: %s\\\\nText: %r\\' % \\\\\\n                (response.status_code,\\n                 time.ctime(expiration_bigip),\\n                 response.url,\\n                 response.text)\\n            raise iControlUnexpectedHTTPError(error_message,\\n                                              response=response)'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.raw_datasets[\"train\"][123456][\"whole_func_string\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T07:52:03.140607300Z",
     "start_time": "2023-08-21T07:52:03.133608400Z"
    }
   },
   "id": "3d59d534f725fdc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "가장 먼저 위 데이터셋을 텍스트 배치를 위해 텍스트 리스트로 만들고 한 번에 메모리에 로딩하지 않기 위해 iterator로 변환해 텍스트 리스트 리스트가 되어야 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d136a8349d393209"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# wrapper.training_corpus = [\n",
    "#     wrapper.raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(wrapper.raw_datasets[\"train\"]), 1000)\n",
    "# ]\n",
    "\n",
    "# 단 한 번만 사용할 수 있는 Python 제너레이터를 사용해 실제로 필요할 때까지 Python이 메모리에 아무것도 로드하지 않도록 설정\n",
    "# 제너레이터 객체를 반환하는 함수로 작성\n",
    "# def get_training_corpus():\n",
    "#     return (\n",
    "#         wrapper.raw_datasets[\"train\"][i: i + 1000][\"whols_func_string\"]\n",
    "#         for i in range(0, len(wrapper.raw_datasets[\"train\"]), 1000)\n",
    "#     )\n",
    "# wrapper.training_corpus = get_training_corpus()\n",
    "\n",
    "# 3가지 방법 중 마지막인 yeild 문 사용\n",
    "def get_training_corpus():\n",
    "    dataset = wrapper.raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx: start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:03:52.763494700Z",
     "start_time": "2023-08-21T08:03:52.750494900Z"
    }
   },
   "id": "b3eb96720f4e7957"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 새로운 토크나이저 학습\n",
    "이제 텍스트 배치의 이터레이터 형태의 말뭉치를 구성했으니 학습할 준비가 되었으므로 GPT-2 모델을 불러온다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153b3a1faeafc27f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a720fcdf2e914e4b852902339518b3aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5685206979484391996c181c79b1aa33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56d742241bb743bdbd234117afd9344f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af670840637e406d8715dc0bbe99719b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "['def',\n 'Ġadd',\n '_',\n 'n',\n 'umbers',\n '(',\n 'a',\n ',',\n 'Ġb',\n '):',\n 'Ċ',\n 'Ġ',\n 'Ġ',\n 'Ġ',\n \"Ġ'\",\n \"''\",\n 'Add',\n 'Ġthe',\n 'Ġtwo',\n 'Ġnumbers',\n 'Ġ`',\n 'a',\n '`',\n 'Ġand',\n 'Ġ`',\n 'b',\n '`',\n \".'\",\n \"''\",\n 'Ċ',\n 'Ġ',\n 'Ġ',\n 'Ġ',\n 'Ġreturn',\n 'Ġa',\n 'Ġ+',\n 'Ġb']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "wrapper.example = \"\"\"def add_numbers(a, b):\n",
    "    '''Add the two numbers `a` and `b`.'''\n",
    "    return a + b\"\"\"\n",
    "wrapper.tokens = wrapper.old_tokenizer.tokenize(wrapper.example)\n",
    "wrapper.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:06:36.679524500Z",
     "start_time": "2023-08-21T08:06:33.711968400Z"
    }
   },
   "id": "2350791684f1393f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 토크나이저는 공백과 줄바꿈을 나타내는 몇가지 특수 기호를 포함하고 있어 비효율적이다. 4칸 혹은 8칸의 공백을 개별 토큰으로 표현하고 _문자가 익숙하지 않은지 함수명이 이상하게 분할된다. 새로운 토크나이저를 학습하고 이러한 문제를 해결하는지 확인하기 위해 `train_new_from_iterator()`를 사용한다.\n",
    "\n",
    "이는, fast tokenizer의 경우에만 동작한다.\n",
    "\n",
    "대부분의 Transformer 모델에는 fast tokenizer가 있으며 **AutoTokenizer**의 경우 항상 fast tokenizer를 선택한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f64d4a16d2308de1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', \"Ġ'''\", 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', \"`.'''\", 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n",
      "27\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# text_iterator: generator of List[str]\n",
    "# vocab_size: int\n",
    "\n",
    "wrapper.tokenizer = wrapper.old_tokenizer.train_new_from_iterator(get_training_corpus(), 52000)\n",
    "\n",
    "wrapper.tokens = wrapper.tokenizer.tokenize(wrapper.example)\n",
    "print(wrapper.tokens)\n",
    "print(len(wrapper.tokens))\n",
    "print(len(wrapper.old_tokenizer.tokenize(wrapper.example)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:17:55.612913700Z",
     "start_time": "2023-08-21T08:16:48.348594700Z"
    }
   },
   "id": "84bf9a460d979bc"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 1.22GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "wrapper.example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(wrapper.tokenizer.tokenize(wrapper.example))\n",
    "\n",
    "del get_training_corpus\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:21:50.765620700Z",
     "start_time": "2023-08-21T08:21:49.354266100Z"
    }
   },
   "id": "13febdebbd7556b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Fast Tokenizer의 특별한 능력\n",
    "\n",
    "## Batch Encoding\n",
    "토크나이저의 출력은 `BatchEncoding` 객체이다. 딕셔너리의 하위클래스지만 fast tokenizer에서 주로 사용하는 추가 메서드가 있다.\n",
    "\n",
    "병렬화(parallelization) 외에도 범위(span)를 항상 추적하는데 이를 offset mapping이라 한다. 차례대로 각 단어를 생성된 토큰에 매핑하거나 원본 텍스트의 각 문자를 내부 토큰에 매핑하거나 그 반대로 매핑하는 것과 같은 기능들이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bea317d2d91cda"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer의 토큰화 결과 타입 <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tokenizer is fast\n",
      "encoding is fast\n",
      "토큰 변환 없이 Access ['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "각 토큰이 유래된 해당 단어의 인덱스 [None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
      "Sylvain\n"
     ]
    }
   ],
   "source": [
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "wrapper.example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "wrapper.encoding = wrapper.tokenizer(wrapper.example)\n",
    "print(\"Tokenizer의 토큰화 결과 타입\", type(wrapper.encoding))\n",
    "print(\"tokenizer is fast\") if wrapper.tokenizer.is_fast else print(\"tokenizer is slow\")\n",
    "print(\"encoding is fast\") if wrapper.encoding.is_fast else print(\"encoding is slow\")\n",
    "\n",
    "# fast tokenizer로 가능한 것들\n",
    "\n",
    "\"\"\"\n",
    "1. 토큰 아이디를 다시 토큰으로 변환하지 않고도 토큰에 액세스할 수 있다.\n",
    "\"\"\"\n",
    "print(\"토큰 변환 없이 Access\", wrapper.encoding.tokens())\n",
    "\n",
    "\"\"\"\n",
    "2. 각 토큰이 유래된 해당 단어의 인덱스를 가져올 수 있다.\n",
    "\n",
    "이는 두 개의 토큰이 같은 단어에 있는지 아니면 토큰이 단어의 시작 부분에 있는지 확인하는데 특히 유용하다.\n",
    "\n",
    "word_ids()와 마찬가지로 sentence_ids()를 통해 토큰을 가져온 문장에 해당 토큰을 매핑할 수도 있다.\n",
    "\"\"\"\n",
    "print(\"각 토큰이 유래된 해당 단어의 인덱스\", wrapper.encoding.word_ids())\n",
    "\n",
    "\"\"\"\n",
    "3. 모든 단어 또는 토큰을 원본 텍스트의 문자에 매핑할 수 있다.\n",
    "word_to_chars()\n",
    "token_to_chars()\n",
    "char_to_word()\n",
    "char_to_token()\n",
    "\"\"\"\n",
    "wrapper.range = wrapper.encoding.word_to_chars(3)\n",
    "print(wrapper.example[wrapper.range[0] : wrapper.range[1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:37:49.306509800Z",
     "start_time": "2023-08-21T23:37:48.947141400Z"
    }
   },
   "id": "1779e37d5276790d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ***token-classification*** 파이프라인의 내부 동작\n",
    "파이프라인의 세 단계 토큰화, 입력 전달, 후처리 중 `token-classification` 파이프라인의 후처리는 조금 더 복잡하다.\n",
    "## 파이프라인으로 기본 실행 결과 도출하기\n",
    "- NER 모델: dbmdz/bert-large-cased-finetuned-conll03-english"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a362bbc1eefe689"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590707, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "# Person, Organization, Location으로 식별\n",
    "wrapper.token_classifier = pipeline(\"token-classification\")\n",
    "print(wrapper.token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "\n",
    "\"\"\"\n",
    "동일한 엔티티에 속하는 토큰을 그룹화\n",
    "- simple: 개체명 내의 각 토큰에 대한 스코어의 평균\n",
    "- first: 각 개체명의 스코어는 해당 개체명의 첫 번째 토큰의 스코어(Sylvain의 경우 토큰 S의 점수)\n",
    "- max: 해당 엔티티 내의 토큰들 중 최대 스코어\n",
    "- average: 해당 항목을 구성하는 단어(토큰 아님) 스코어의 평균\n",
    "\"\"\"\n",
    "wrapper.token_classifier = pipeline(\"token-classification\", aggregation_strategy = \"simple\")\n",
    "print(wrapper.token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:37:58.261067500Z",
     "start_time": "2023-08-21T23:37:52.436087500Z"
    }
   },
   "id": "39116cc57fbb45b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## inputs에서 predictions까지\n",
    "위의 예제를 pipeline() 없이 동일한 결과 얻기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed4bec50a753ee65"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.4973, -2.3986, -1.3582, -2.7887,  0.7575, -1.8873,  0.4344,\n",
      "          -1.9900, -0.3397],\n",
      "         [ 9.4719, -2.2261, -0.9849, -2.6116,  0.1219, -2.0627, -0.1259,\n",
      "          -1.8758, -0.0609],\n",
      "         [ 9.8670, -2.2175, -1.3125, -2.4866, -0.2550, -1.8536,  0.0856,\n",
      "          -1.7520, -0.6437],\n",
      "         [-0.2011, -2.1873, -1.5316, -2.7110,  8.4025, -2.4168, -0.6980,\n",
      "          -3.0337, -0.0997],\n",
      "         [ 0.1065, -2.0520, -1.4787, -2.8139,  7.4525, -2.8399, -0.0626,\n",
      "          -3.3666, -0.4683],\n",
      "         [ 0.5985, -2.2538, -1.1926, -3.0111,  7.0070, -2.8675,  0.3492,\n",
      "          -3.3129, -0.2878],\n",
      "         [-0.0584, -2.2660, -1.4335, -3.1940,  8.3225, -2.6212, -0.0348,\n",
      "          -2.9780, -0.2957],\n",
      "         [ 9.6889, -2.4281, -1.5653, -2.5225, -0.9693, -1.5668,  0.4285,\n",
      "          -1.9413, -0.6774],\n",
      "         [ 9.0116, -2.1216, -1.4140, -2.6964,  0.2728, -1.7851,  0.3635,\n",
      "          -1.8407, -0.5922],\n",
      "         [ 9.5258, -2.2616, -1.4557, -2.9603, -0.1311, -1.7799,  0.9169,\n",
      "          -2.2549, -0.9692],\n",
      "         [ 9.1087, -2.2834, -1.3437, -2.8742, -0.2521, -1.5712,  1.1501,\n",
      "          -2.0786, -0.8658],\n",
      "         [ 2.5185, -3.1537, -1.6923, -3.4240,  1.4335, -1.8089,  6.5008,\n",
      "          -3.0264, -0.2619],\n",
      "         [ 1.7707, -2.4992, -0.1088, -3.2825,  0.4034, -1.4262,  5.9701,\n",
      "          -2.6502, -0.1259],\n",
      "         [ 0.6466, -2.9276, -0.1020, -3.0776,  0.7036, -1.2746,  6.3889,\n",
      "          -2.7266,  0.3822],\n",
      "         [ 9.2571, -2.6779, -1.2145, -2.7276, -0.9370, -1.5445,  1.1025,\n",
      "          -1.8477, -0.3661],\n",
      "         [-0.2206, -2.5108, -1.2976, -2.9758, -0.5795, -2.2071,  1.8236,\n",
      "          -1.6484,  7.0975],\n",
      "         [ 8.7507, -2.2626, -1.5300, -2.2890, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "wrapper.model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.model_checkpoint)\n",
    "wrapper.model = AutoModelForTokenClassification.from_pretrained(wrapper.model_checkpoint)\n",
    "\n",
    "wrapper.example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "wrapper.inputs = wrapper.tokenizer(wrapper.example, return_tensors = \"pt\")\n",
    "wrapper.outputs = wrapper.model(**wrapper.inputs)\n",
    "print(wrapper.outputs)\n",
    "print(wrapper.inputs[\"input_ids\"].shape)\n",
    "print(wrapper.outputs.logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:38:09.558164800Z",
     "start_time": "2023-08-21T23:38:07.548310Z"
    }
   },
   "id": "88a3378d0a267419"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델에는 9개의 서로 다른 레이블이 존재하므로 1 X 19의 input이 1 X 19 X 9의 logit을 갖는다.\n",
    "\n",
    "`softmax`를 사용해 logits를 확률로 변환하고 argmax를 사용해 예측 결과를 얻을 수 있다.(softmax는 순서를 변경하지 않기 때문에 logits에 대해서 argmax를 취할 수 있다.)\n",
    "\n",
    "`model.config.id2label` 속성에는 예측 결과를 확인하는데 사용할 수 있는 레이블에 대한 인덱스 매핑이 포함되어있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f808f287c8bbb2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9994322657585144, 1.6470316040795296e-05, 3.426706462050788e-05, 1.6042342394939624e-05, 8.250699465861544e-05, 2.1382335035013966e-05, 0.00015649119450245053, 1.965213414223399e-05, 0.0002208926307503134], [0.9989631175994873, 1.8515736883273348e-05, 5.240452446741983e-05, 1.2534720553958323e-05, 0.0004347364301793277, 3.087432560278103e-05, 0.00031468752422370017, 2.78607003565412e-05, 0.00014510865730699152], [0.999708354473114, 8.308118594868574e-06, 2.8745585950673558e-05, 5.6503527048334945e-06, 8.694847929291427e-05, 9.783467248780653e-06, 6.786138692405075e-05, 1.1793980775109958e-05, 7.241893035825342e-05], [0.9998350143432617, 5.645536475640256e-06, 1.3955152098787948e-05, 4.3133773033332545e-06, 4.017695027869195e-05, 8.123070074361749e-06, 5.648490696330555e-05, 8.99163478607079e-06, 2.7239138944423757e-05], [0.00018333422485738993, 2.515664164093323e-05, 4.8462032282259315e-05, 1.4900581845722627e-05, 0.9993828535079956, 1.999776031880174e-05, 0.00011153631203342229, 1.0790769920276944e-05, 0.00020288894302211702], [0.0006440267316065729, 7.437886961270124e-05, 0.0001319660514127463, 3.4719600080279633e-05, 0.9981548190116882, 3.382965223863721e-05, 0.0005438167136162519, 1.9978177078883164e-05, 0.00036244612419977784], [0.001640839851461351, 9.469484211876988e-05, 0.00027364352717995644, 4.4406413508113474e-05, 0.995907187461853, 5.1262213673908263e-05, 0.001278792624361813, 3.2835454476298764e-05, 0.0006763276760466397], [0.00022901801276020706, 2.518332257750444e-05, 5.7899465900845826e-05, 9.95701066131005e-06, 0.9992327690124512, 1.7655056581133977e-05, 0.00023448346473742276, 1.2356580555206165e-05, 0.00018065006588585675], [0.999804675579071, 5.4650263336952776e-06, 1.2949992196809035e-05, 4.972429451299831e-06, 2.35032075579511e-05, 1.2930702723679133e-05, 9.509907249594107e-05, 8.891754077922087e-06, 3.1469306122744456e-05], [0.9995046854019165, 1.4611873666581232e-05, 2.9646907933056355e-05, 8.223405529861338e-06, 0.0001601653202669695, 2.045680594164878e-05, 0.00017537118401378393, 1.9349117792444304e-05, 6.743980338796973e-05], [0.9996776580810547, 7.596964678668883e-06, 1.7006927009788342e-05, 3.777614665523288e-06, 6.396081153070554e-05, 1.2297879038669635e-05, 0.00018241429643239826, 7.64841115596937e-06, 2.7664662411552854e-05], [0.999434769153595, 1.1278328202024568e-05, 2.8862716135336086e-05, 6.246597422432387e-06, 8.598280692240223e-05, 2.2989384888205677e-05, 0.00034945193328894675, 1.3841326108376961e-05, 4.654669464798644e-05], [0.0181562602519989, 6.24591630185023e-05, 0.0002693218702916056, 4.766615529661067e-05, 0.006134606432169676, 0.0002396744239376858, 0.9738931059837341, 7.093795284163207e-05, 0.0011259125312790275], [0.014645998366177082, 0.00020479795057326555, 0.002236028900370002, 9.35708376346156e-05, 0.0037316353991627693, 0.0005988667835481465, 0.9761149883270264, 0.00017609857604838908, 0.0021980940364301205], [0.003171559190377593, 8.892147161532193e-05, 0.0015001408755779266, 7.652998465346172e-05, 0.003357555018737912, 0.0004643812426365912, 0.9887974858283997, 0.00010871316771954298, 0.0024345857091248035], [0.9995326995849609, 6.553966159117408e-06, 2.8316171665210277e-05, 6.235947694221977e-06, 3.7372072256403044e-05, 2.0357150788186118e-05, 0.00028727532480843365, 1.5032640476420056e-05, 6.614298763452098e-05], [0.0006589225959032774, 6.67124695610255e-05, 0.0002244396455353126, 4.190392428427003e-05, 0.0004602030385285616, 9.038783173309639e-05, 0.005088841542601585, 0.00015803203859832138, 0.99321049451828], [0.9994322657585144, 1.64706762006972e-05, 3.4267621231265366e-05, 1.6042449715314433e-05, 8.250888640759513e-05, 2.1382335035013966e-05, 0.0001564932899782434, 1.965230330824852e-05, 0.00022089369304012507], [0.9994322657585144, 1.6470268747070804e-05, 3.4266999136889353e-05, 1.6042311472119763e-05, 8.250675455201417e-05, 2.1382315026130527e-05, 0.00015649090346414596, 1.9652115952339955e-05, 0.00022089220874477178]]\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "wrapper.probabilities = torch.nn.functional.softmax(wrapper.outputs.logits, dim = -1)[0].tolist()\n",
    "wrapper.predictions = wrapper.outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(wrapper.probabilities)\n",
    "print(wrapper.predictions)\n",
    "\n",
    "\"\"\"\n",
    "- O: 개체명에 포함X, outside\n",
    "- MISC: 기타, miscellaneous\n",
    "- PER: 인물, person\n",
    "- ORG: 조직, organization\n",
    "- LOC: 지명, location\n",
    "\n",
    "- B-XXX: 토큰이 엔티티의 시작부분에 있음을 나타냄\n",
    "- I-XXX: 토큰이 엔티티의 내부에 있음을 나타냄\n",
    "\"\"\"\n",
    "print(wrapper.model.config.id2label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:46:18.359499600Z",
     "start_time": "2023-08-21T23:46:18.350499700Z"
    }
   },
   "id": "9829eed49a2502a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "O로 분류되지 않은 각 토큰의 점수와 레이블만 가져오기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e80dbd49408fd59"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'},\n {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'},\n {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'},\n {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.results = []\n",
    "wrapper.tokens = wrapper.inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(wrapper.predictions):\n",
    "    label = wrapper.model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        wrapper.results.append(\n",
    "\t\t\t{\"entity\": label, \"score\": wrapper.probabilities[idx][pred], \"word\": wrapper.tokens[idx]}\n",
    "        )\n",
    "\n",
    "wrapper.results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:51:13.150661100Z",
     "start_time": "2023-08-21T23:51:13.134661400Z"
    }
   },
   "id": "f84bd3d0fe0fbbc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 결과는 파이프라인과 매우 유사하지만 한가지 차이점이 있다면 원본 문장에서 각 엔티티의 시작과 끝에 대한 정보를 제공한 파이프라인과 달리 위 코드는 그렇지않다.\n",
    "Offset Mapping을 적용하기 위해 입력에 토크나이저 적용 시 `return_offsets_mapping=True`를 설정한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ceeb5e70e913069e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'I-PER',\n  'score': 0.9993828535079956,\n  'word': 'S',\n  'start': 11,\n  'end': 12},\n {'entity': 'I-PER',\n  'score': 0.9981548190116882,\n  'word': '##yl',\n  'start': 12,\n  'end': 14},\n {'entity': 'I-PER',\n  'score': 0.995907187461853,\n  'word': '##va',\n  'start': 14,\n  'end': 16},\n {'entity': 'I-PER',\n  'score': 0.9992327690124512,\n  'word': '##in',\n  'start': 16,\n  'end': 18},\n {'entity': 'I-ORG',\n  'score': 0.9738931059837341,\n  'word': 'Hu',\n  'start': 33,\n  'end': 35},\n {'entity': 'I-ORG',\n  'score': 0.9761149883270264,\n  'word': '##gging',\n  'start': 35,\n  'end': 40},\n {'entity': 'I-ORG',\n  'score': 0.9887974858283997,\n  'word': 'Face',\n  'start': 41,\n  'end': 45},\n {'entity': 'I-LOC',\n  'score': 0.99321049451828,\n  'word': 'Brooklyn',\n  'start': 49,\n  'end': 57}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.inputs_with_offsets = wrapper.tokenizer(wrapper.example, return_offsets_mapping = True)\n",
    "\n",
    "wrapper.results = []\n",
    "wrapper.tokens = wrapper.inputs_with_offsets.tokens()\n",
    "wrapper.offsets = wrapper.inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(wrapper.predictions):\n",
    "    label = wrapper.model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        start, end = wrapper.offsets[idx]\n",
    "        wrapper.results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": wrapper.probabilities[idx][pred],\n",
    "                \"word\": wrapper.tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            }\n",
    "        )\n",
    "\n",
    "wrapper.results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T23:57:51.891447700Z",
     "start_time": "2023-08-21T23:57:51.887449Z"
    }
   },
   "id": "836a27ac745c83ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entity 그룹화\n",
    "엔티티 토큰들을 합쳐 단어를 만들때 ##를 지우고 ##로 시작하지 않으면 공백으로 연결하는 등의 지저분한 코드가 필요하지만 오프셋을 사용하는 경우 모든 사용자 정의 코드가 사라지고 slicing으로 해결 가능하다.\n",
    "```python\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    "example[33:45] # \"Hugging Face\"\n",
    "```\n",
    "\n",
    "특정 엔티티에 포함된 토큰들을 그룹화하는 동안 예측 결과를 후처리하는 코드를 작성하기 위해 B-XXX 또는 I-XXX로 레이블이 지정될 수 있는 첫 번째 엔티티를 제외하고 연속적이고 I-XXX로 레이블이 지정된 엔티티를 함께 그룹화한다. 따라서, 그룹화 도중 다음 토큰이 'O' 또는 B-XXX이거나 새로운 유형의 토큰으로 시작하는 경우 그룹화를 중지한다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d7337a5f31cc96"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.998169407248497, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796018600463867, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321049451828, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 0.97GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "wrapper.results = []\n",
    "wrapper.inputs_with_offsets = wrapper.tokenizer(wrapper.example, return_offsets_mapping=True)\n",
    "wrapper.tokens = wrapper.inputs_with_offsets.tokens()\n",
    "wrapper.offsets = wrapper.inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "wrapper.idx = 0\n",
    "while wrapper.idx < len(wrapper.predictions):\n",
    "    pred = wrapper.predictions[wrapper.idx]\n",
    "    label = wrapper.model.config.id2label[pred]\n",
    "    if label == 'O':\n",
    "        wrapper.idx += 1\n",
    "        continue\n",
    "    \n",
    "    # Label의 \"B-\", \"I-\" 지우기\n",
    "    label = label[2:]\n",
    "    start, end = wrapper.offsets[wrapper.idx]\n",
    "    \n",
    "    # I-label 토큰 전부 모으기\n",
    "    all_scores = []\n",
    "    while (\n",
    "        wrapper.idx < len(wrapper.predictions)\n",
    "        and wrapper.model.config.id2label[wrapper.predictions[wrapper.idx]] == f\"I-{label}\"\n",
    "    ):\n",
    "        all_scores.append(wrapper.probabilities[wrapper.idx][pred])\n",
    "        _, end = wrapper.offsets[wrapper.idx]\n",
    "        wrapper.idx += 1\n",
    "    \n",
    "    # Score는 그룹 엔티티 내의 토큰 스코어의 평균\n",
    "    score = np.mean(all_scores).item()\n",
    "    word = wrapper.example[start:end]\n",
    "    wrapper.results.append(\n",
    "\t\t{\n",
    "            \"entity_group\": label,\n",
    "            \"score\": score,\n",
    "            \"word\": word,\n",
    "            \"start\": start,\n",
    "            \"end\": end\n",
    "        }\n",
    "    )\n",
    "    wrapper.idx += 1\n",
    "\n",
    "print(wrapper.results)\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T00:28:13.058631900Z",
     "start_time": "2023-08-22T00:28:12.160077400Z"
    }
   },
   "id": "8e49de4d754619b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. QA 파이프라인에서의 fast tokenizer\n",
    "`question-answering` 파이프라인을 살펴보고 오프셋을 활용하여 컨텍스트에서 입력 질문에 대한 답변을 직접 구하는 방법을 살펴본 후 절단(truncate)될 수밖에 없는 매우 긴 컨텍스트를 처리하는 방법을 본다.\n",
    "\n",
    "## **question-answering** 파이프라인"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51b12daf6dcb2313"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61da58c07d024233aa939fd653ef0d1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "012f8ff6d07343d9b486e0549e277c90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ce1c8d049fa460bb8eb89b646f61a4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6e019b85f654ffcb65ceb99d4a18458"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c88ddb8792e24cd9becf861c13fe4933"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'score': 0.9898537993431091,\n 'start': 86,\n 'end': 114,\n 'answer': 'Jax, PyTorch, and TensorFlow'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.question_answerer = pipeline(\"question-answering\")\n",
    "wrapper.context = \"\"\"\n",
    "&#129303; Transformers is backed by the three most popular deep learning libraries - Jax, PyTorch, and TensorFlow - with a seamless integration between them. It`s straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "wrapper.question = \"Which deep learning libraries back &#129303; Transformers?\"\n",
    "wrapper.question_answerer(question = wrapper.question, context = wrapper.context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T00:38:13.082905400Z",
     "start_time": "2023-08-22T00:38:03.547129700Z"
    }
   },
   "id": "7068fa290802ca25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델이 허용하는 최대 길이보다 긴 텍스트를 자르거나 분할할 수 없는 다른 파이프라인과 달리 이 파이프라인은 매우 긴 컨텍스트를 처리할 수 있으며 질문에 대한 답이 컨텍스트의 마지막에 있더라도 그 답변을 추출할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97784ad5db389cd"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.9886583685874939,\n 'start': 1958,\n 'end': 1985,\n 'answer': 'Jax, PyTorch and TensorFlow'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.long_context = \"\"\"\n",
    " &#129303; Transformers: State of the Art NLP\n",
    "\n",
    " &#129303; Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    " &#129303; Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n",
    " \n",
    " Why should I use transformers?\n",
    " \n",
    " 1. Easy-to-use state-of-the-art models:\n",
    "    - High performance on NLU and NLG tasks.\n",
    "    - Low barrier to entry for educators and practitioners.\n",
    "    - Few user-facing abstractions with just three classes to learn.\n",
    "    - A unified API for using all our pretrained models.\n",
    "    - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "    - Practitioners can reduce compute time and production costs.\n",
    "    - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model`s lifetime:\n",
    "    - Train state-of-the-art models in 3 lines of code.\n",
    "    - Move a single model between TF2.0/Pytorch frameworkd at will.\n",
    "    - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "    - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "    - Model internals are exposed as consistently as possible.\n",
    "    - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    " &#129303; Transformers is backed by the three most popular deep learning libraries = Jax, PyTorch and TensorFlow - with a weamless integration between them. It`s straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "wrapper.question_answerer(question = wrapper.question, context = wrapper.long_context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T00:54:11.807059500Z",
     "start_time": "2023-08-22T00:54:11.657759600Z"
    }
   },
   "id": "5f37cdcce5f6cae1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 질의 응답을 위한 사전학습 모델 사용하기\n",
    "디폴트 체크포인트: distilbert-base-cased-distilled-squad"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b61a5e134d1aa2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77]) torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "wrapper.model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.model_checkpoint)\n",
    "wrapper.model = AutoModelForQuestionAnswering.from_pretrained(wrapper.model_checkpoint)\n",
    "\n",
    "wrapper.inputs = wrapper.tokenizer(wrapper.question, wrapper.context, return_tensors = \"pt\")\n",
    "wrapper.outputs = wrapper.model(**wrapper.inputs)\n",
    "\n",
    "# 질의 응답 모델은 다른 모델과 다르게 하나의 로짓 텐서를 반환하지 않고 정답의 시작 토큰과 정답의 마지막 토큰에 해당하는 로짓 텐서를 반환한다.\n",
    "wrapper.start_logits = wrapper.outputs.start_logits\n",
    "wrapper.end_logits = wrapper.outputs.end_logits\n",
    "print(wrapper.start_logits.shape, wrapper.end_logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T01:08:07.538604100Z",
     "start_time": "2023-08-22T01:08:06.493657Z"
    }
   },
   "id": "e43347d1d71693f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이러한 로짓들을 확률로 변환하기 위해 `softmax` 함수를 적용하기 전에 context가 아닌 토큰 인덱스를 masking해야 한다. 입력이 `[CLS] question [SEP] context [SEP]`이므로 질문에 포함된 토큰과 [SEP] 토큰을 마스킹하고 일부 모델에서는 context에 답이 없음을 나타내기 위해 사용할 수도 있으므로 [CLS] 토큰은 마스킹하지 않는다.\n",
    "\n",
    "나중에 `softmax`를 적용할 것이기 때문에 masking하려는 로짓을 큰 음수로 바꾼다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e70a1824b34b13f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼각행렬 크기: torch.Size([77, 77])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[CLS] question [SEP] context [SEP]\n",
    " => [None, 0 ... 0, None, 1 ... 1, None]\n",
    "- [CLS], [SEP] 토큰은 None\n",
    "- 첫 번째 입력인 question은 0\n",
    "- 두 번째 입력인 context는 1\n",
    "\"\"\"\n",
    "wrapper.sequence_ids = wrapper.inputs.sequence_ids()\n",
    "\n",
    "# 컨텍스트 토큰들을 제외하고는 모두 마스킹한다.\n",
    "wrapper.mask = [i != 1 for i in wrapper.sequence_ids]\n",
    "\n",
    "# [CLS] 토큰은 마스킹하지 않는다.\n",
    "wrapper.mask[0] = False\n",
    "wrapper.mask = torch.tensor(wrapper.mask)[None]\n",
    "# print(wrapper.mask)\n",
    "\n",
    "# [CLS] 토큰과 context를 제외한 나머지 토큰들은 전부 -10000으로 masking\n",
    "wrapper.start_logits[wrapper.mask] = -10000\n",
    "wrapper.end_logits[wrapper.mask] = -10000\n",
    "\n",
    "# masking을 완료했으니 `softmax`를 적용한다.\n",
    "wrapper.start_probabilities = torch.nn.functional.softmax(wrapper.start_logits, dim=-1)[0]\n",
    "wrapper.end_probabilities = torch.nn.functional.softmax(wrapper.end_logits, dim=-1)[0]\n",
    "# print(f\"softmax 적용 결과: {wrapper.start_probabilities}\")\n",
    "\n",
    "\"\"\"\n",
    "start_probabilities로 예측한 시작 인덱스가 end_probabilities로 예측한 종료 인덱스보다 클 수 있으므로 추가 작업이 필요하다.\n",
    "\n",
    "1. `start_idx <= end_idx`를 만족하는 가능한 start_idx 및 end_idx의 확률을 계산한 다음 가장 높은 확률을 가진 튜플(start_index, end_index)을 선택한다.\n",
    "    - 답변이 start_index에서 시작해 end_index에서 끝날 확률은 두 확률의 곱과 같다.\n",
    "    - 따라서, `start_index <= end_index`를 만족하는 모든 `start_index * end_index`를 계산하면 된다.\n",
    "\"\"\"\n",
    "# x = torch.tensor([1, 2, 3]) ---- tensor([1, 2, 3])\n",
    "# y = x[:, None] ----------------- tensor([[1], [2], [3]])\n",
    "# (77 X 77) = (77 X 1) X (1 X 77)\n",
    "wrapper.scores = wrapper.start_probabilities[:, None] * wrapper.end_probabilities[None, :]\n",
    "# print(f\"start_index * end_index = {wrapper.scores}\")\n",
    "\n",
    "# start_index < end_index를 만족하는 값들을 0으로 설정하여 마스킹(나머지는 모두 양수).\n",
    "# torch.triu() 함수는 인수로 전달된 2D tensor의 위쪽 삼각형 부분(삼각행렬)을 반환하므로 해당 마스킹을 수행할 수 있음.\n",
    "wrapper.scores = torch.triu(wrapper.scores)\n",
    "# print(f\"masking: {wrapper.scores}\")\n",
    "print(f\"삼각행렬 크기: {wrapper.scores.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:03:43.034351400Z",
     "start_time": "2023-08-22T02:03:43.011847500Z"
    }
   },
   "id": "bdca1e69335915b8"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 scores: 0.98985356092453\n"
     ]
    }
   ],
   "source": [
    "# 최대값 인덱스 구하기\n",
    "# argmax()를 이용해 최대값의 인덱스를 가져옴(PyTorch는 평탄화된 인덱스만 가져옴)\n",
    "wrapper.max_index = wrapper.scores.argmax().item()\n",
    "\n",
    "wrapper.start_index = wrapper.max_index // wrapper.scores.shape[1]\n",
    "wrapper.end_index = wrapper.max_index % wrapper.scores.shape[1]\n",
    "print(f\"결과 scores: {wrapper.scores[wrapper.start_index, wrapper.end_index]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:08:30.512836400Z",
     "start_time": "2023-08-22T02:08:30.504838Z"
    }
   },
   "id": "767cd46fcb250205"
  },
  {
   "cell_type": "markdown",
   "source": [
    "응답들의 토큰 단위 `start_index`와 `end_index`를 구했기 때문에 이를 기반으로 이제 context 내 문자 단위 인덱스로 변환해야 한다. 여기서 offset이 매우 유용할 것이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13e142c867bf6028"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 86, 'end': 114, 'score': tensor(0.9899, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "wrapper.inputs_with_offsets = wrapper.tokenizer(wrapper.question, wrapper.context, return_offsets_mapping=True)\n",
    "wrapper.offsets = wrapper.inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "wrapper.start_char, _ = wrapper.offsets[wrapper.start_index]\n",
    "_, wrapper.end_char = wrapper.offsets[wrapper.end_index]\n",
    "\n",
    "wrapper.result = {\n",
    "    \"answer\": wrapper.context[wrapper.start_char:wrapper.end_char],\n",
    "    \"start\": wrapper.start_char,\n",
    "    \"end\": wrapper.end_char,\n",
    "    \"score\": wrapper.scores[wrapper.start_index, wrapper.end_index]\n",
    "}\n",
    "print(wrapper.result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:14:22.107491800Z",
     "start_time": "2023-08-22T02:14:22.080491100Z"
    }
   },
   "id": "2d7a9dac65e2bc37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 길이가 긴 context 다루기\n",
    "위에서 사용한 질문과 long_context를 토큰화해보면 `question-answering`파이프라인에서 사용된 최대 길이 384보다 더 많은 토큰들이 출력된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89db7a622ac28665"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n"
     ]
    }
   ],
   "source": [
    "wrapper.inputs = wrapper.tokenizer(wrapper.question, wrapper.long_context)\n",
    "print(len(wrapper.inputs[\"input_ids\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:15:59.434361900Z",
     "start_time": "2023-08-22T02:15:59.410407200Z"
    }
   },
   "id": "7f9992f52ed0e536"
  },
  {
   "cell_type": "markdown",
   "source": [
    "따라서 최대 길이만큼 입력을 truncate해야 하는데 이는 토크나이저에 `truncation=\"only_second\"` 인수를 주면 된다. 하지만 이는 컨텍스트의 뒷부분에 정답이 있다면 답변을 찾을 수 없다. 이를 해결하기 위해 `question-answering`파이프라인은 context를 더 작은 청크로 분할하여 최대 길이를 지정할 수 있다. 정답을 찾을 수 있도록 context를 잘못된 위치에서 분할하지 않도록 하기 위해 청크 사이에 약간의 overlap도 포함한다. `return_overflowing_tokens=True`, `stride`인수로 겹침 정도를 지정할 수 있다.\n",
    "\n",
    "`inputs[\"input_ids\"]`의 각 항목이 최대 6개의 토큰을 갖는 청크로 분할하고 마지막 항목이 다른 항목과 같은 크기가 되도록 `padding`을 추가한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63cfdfb1d17d778a"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n",
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n",
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "wrapper.sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "wrapper.inputs = wrapper.tokenizer(\n",
    "    wrapper.sentence, truncation = True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "for ids in wrapper.inputs[\"input_ids\"]:\n",
    "    print(wrapper.tokenizer.decode(ids))\n",
    "\n",
    "print(wrapper.inputs.keys())\n",
    "\n",
    "# 각 결과가 어느 문장에 해당하는지 알려주는 map\n",
    "print(wrapper.inputs[\"overflow_to_sample_mapping\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:22:46.318627700Z",
     "start_time": "2023-08-22T02:22:46.311626800Z"
    }
   },
   "id": "5417ba4e5942438f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "long_context로 돌아가보면 다음과 같다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab2144047a694e52"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "wrapper.inputs = wrapper.tokenizer(\n",
    "    wrapper.question,\n",
    "    wrapper.long_context,\n",
    "    stride = 128,\n",
    "    max_length = 384,\n",
    "    padding = \"longest\",\n",
    "    truncation = \"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "# model에서 사용하지 않는 매개변수 제거\n",
    "_ = wrapper.inputs.pop(\"overflow_to_sample_mapping\")\n",
    "wrapper.offsets = wrapper.inputs.pop(\"offset_mapping\")\n",
    "\n",
    "wrapper.inputs = wrapper.inputs.convert_to_tensors(\"pt\")\n",
    "print(wrapper.inputs[\"input_ids\"].shape)\n",
    "\n",
    "# 길이가 긴 컨텍스트는 두 개로 분할되었으며 모델의 출력은 시작 및 마지막 로짓으로 구성된다.\n",
    "wrapper.outputs = wrapper.model(**wrapper.inputs)\n",
    "wrapper.start_logits = wrapper.outputs.start_logits\n",
    "wrapper.end_logits = wrapper.outputs.end_logits\n",
    "print(wrapper.start_logits.shape, wrapper.end_logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:32:50.240343600Z",
     "start_time": "2023-08-22T03:32:50.084440300Z"
    }
   },
   "id": "eddcb7ed12f91b27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "전과 마찬가지로 `softmax`를 취하기 전에 context의 일부가 아닌 토큰을 먼저 마스킹한다. 또한 모든 패딩 토큰을 마스킹한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b31bad7998ebfea7"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "wrapper.sequence_ids = wrapper.inputs.sequence_ids()\n",
    "\n",
    "wrapper.mask = [i != 1 for i in wrapper.sequence_ids]\n",
    "\n",
    "# Unmask [CLS]\n",
    "wrapper.mask[0] = False\n",
    "\n",
    "# Mask [PAD]\n",
    "wrapper.mask = torch.logical_or(torch.tensor(wrapper.mask)[None], (wrapper.inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "# -10000으로 masking\n",
    "wrapper.start_logits[wrapper.mask] = -10000\n",
    "wrapper.end_logits[wrapper.mask] = -10000\n",
    "\n",
    "# softmax를 사용해 logits을 확률로 변환\n",
    "wrapper.start_probabilities = torch.nn.functional.softmax(wrapper.start_logits, dim=-1)\n",
    "wrapper.end_probabilities = torch.nn.functional.softmax(wrapper.end_logits, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:46:52.645742900Z",
     "start_time": "2023-08-22T03:46:52.642742900Z"
    }
   },
   "id": "6afac60a469485fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "다음 단계는 길이가 짧은 컨텍스트에 대해 수행한 작업과 유사하지만 청크가 2개이므로 2번 반복한다.\n",
    "\n",
    "가능한 모든 답변에 점수를 부여한 다음 가장 좋은 점수를 받은 답변을 선택한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ad08130529273c"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 28, 0.7921538949012756), (202, 213, 0.9886583685874939)]\n"
     ]
    }
   ],
   "source": [
    "wrapper.candidates = []\n",
    "for start_probs, end_probs in zip(wrapper.start_probabilities, wrapper.end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :] # 384 X 384\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "    \n",
    "    start_idx = idx // scores.shape[0]\n",
    "    end_idx = idx % scores.shape[0]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    \n",
    "    wrapper.candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(wrapper.candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:54:34.107839600Z",
     "start_time": "2023-08-22T03:54:34.075591800Z"
    }
   },
   "id": "cab537cd3311b225"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'State of the Art NLP', 'start': 26, 'end': 46, 'score': 0.7921538949012756}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1958, 'end': 1985, 'score': 0.9886583685874939}\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 1.01GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "# 2개의 청크 보유\n",
    "# candidates: (2, 3) offsets: (2, 384)\n",
    "for candidate, offset in zip(wrapper.candidates, wrapper.offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    result = {\n",
    "        \"answer\": wrapper.long_context[start_char:end_char],\n",
    "        \"start\": start_char,\n",
    "        \"end\": end_char,\n",
    "        \"score\": score\n",
    "    }\n",
    "    print(result)\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T04:04:13.225958Z",
     "start_time": "2023-08-22T04:04:12.712284Z"
    }
   },
   "id": "40572c5e2d3a9f36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Normalization 및 Pre-tokenization\n",
    "\n",
    "## Normalization, 정규화\n",
    "공백 제거, 고문자 변환 악센트 제거 등과 같은 일반적인 정제 작업이 포함된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd12a4bdb9e8fab3"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Tokenizer: <class 'tokenizers.Tokenizer'>\n",
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# bert-base-cased 모델을 사용해 소문자 변환 후 악센트 제거\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"Backend Tokenizer: {type(wrapper.tokenizer.backend_tokenizer)}\")\n",
    "print(wrapper.tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T04:09:17.768652600Z",
     "start_time": "2023-08-22T04:09:17.537006700Z"
    }
   },
   "id": "6405433a3eb7825c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-tokenization, 사전토큰화\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299bca7a15a72bba"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased: [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]\n",
      "gpt2: [('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)), ('?', (19, 20))]\n",
      "t5-small: [('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]\n"
     ]
    }
   ],
   "source": [
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"bert-base-uncased: {wrapper.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str('Hello, how are  you?')}\")\n",
    "\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"gpt2: {wrapper.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str('Hello, how are  you?')}\")\n",
    "\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "print(f\"t5-small: {wrapper.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str('Hello, how are  you?')}\")\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T04:13:21.943147Z",
     "start_time": "2023-08-22T04:13:20.485301800Z"
    }
   },
   "id": "d26697cc2312a688"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SentencePiece\n",
    "BPE 토크나이저 섹션에서 볼 모든 모델과 함께 사용할 수 있는 텍스트 전처리를 위한 토큰화 알고리즘.\n",
    "\n",
    "텍스트를 일련의 유니코드 문자로 간주하고 공백을 특수문자인 _로 치환. Unigram과 함께 사용하면 사전토큰화 단계가 필요하지 않으므로 공백 문자가 사용되지 않는 일본어나 중국어에 매우 유용하다.\n",
    "\n",
    "SentencePiece의 또 다른 주요 기능은 가역적 토큰화이다. 공백에 대한 특별한 처리가 없기 때문에 토큰 디코딩은 토큰을 연결하고 _를 공백으로 바꾸는 것으로 간단히 수행되어 정규화된 텍스트가 출력된다.\n",
    "\n",
    "## 토큰화 알고리즘 개요\n",
    "![](../assets/tokenization.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c72cd888ed4e83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
