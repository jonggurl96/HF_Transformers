{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2장. &#129303;Transformers 라이브러리 사용하기\n",
    "- [강좌 링크](https://wikidocs.net/166794)\n",
    "\n",
    "1장에서 보았듯 Transformer 모델은 규모가 매우 커 이를 해결하기 위해 &#129303;Transformers 라이브러리가 만들어지게 되었고 특징은 다음과 같다.\n",
    "- 사용 용이성, Ease of use\n",
    "- 유연성, Flexibility\n",
    "- 단순성, Simplicity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abf64f86846b3399"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Pipeline 내부 실행 과정\n",
    "1. 전처리(Preprocessing)\n",
    "2. Model\n",
    "3. 후처리(Postprocessing)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312dd26d6f535bd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer를 이용한 Preprocessing\n",
    "\n",
    "다른 신경망과 마찬가지로 Transformer 모델 또한 원시 텍스트를 직접 처리할 수 없으므로 모델이 이해할 수 있는 숫자로 바꿔주게 됨.\n",
    "- 입력을 token이라 불리는 { word | subword | symbol }로 분할\n",
    "- 각 token을 정수로 mapping\n",
    "- 모델에 유용할 수 있는 additional inputs를 추가\n",
    "\n",
    "이 모든 preprocess는 모델이 pretraining될 때와 정확히 동일한 방식으로 수행되어야 하므로 Model Hub에서 해당 정보를 다운로드해야 함."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c619fc23b5b4af1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from custom_utils import *\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "wrapper = CustomObject()\n",
    "\n",
    "# sentiment-analysis 파이프라인의 default checkpoint\n",
    "wrapper.checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(wrapper.checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:16:36.497572400Z",
     "start_time": "2023-08-10T04:16:35.121586200Z"
    }
   },
   "id": "1a6afa1c9309f185"
  },
  {
   "cell_type": "markdown",
   "source": [
    "한 번 Tokenizer를 생성하면 아래 코드처럼 문장을 입력하여 모델에 바로 전달할 수 있는 Python Dictionary 정보를 구할 수 있음.\n",
    "\n",
    "이후 해야할 일은 input_ids 리스트를 텐서로 변환하는 것뿐임."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba2d1f59d8c355c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "wrapper.raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\"\n",
    "]\n",
    "wrapper.inputs = wrapper.tokenizer(wrapper.raw_inputs, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "print(wrapper.inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:18:58.872298700Z",
     "start_time": "2023-08-10T04:18:58.860100300Z"
    }
   },
   "id": "4f2851ed2777c94b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 살펴보기\n",
    "\n",
    "Tokenizer와 동일한 방법으로 다운로드 가능.\n",
    "\n",
    "AutoModel의 from_pretrained() 활용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25947a198a439717"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "wrapper.model = AutoModel.from_pretrained(wrapper.checkpoint)\n",
    "print(wrapper.model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:28:02.485426500Z",
     "start_time": "2023-08-10T04:28:02.018944300Z"
    }
   },
   "id": "1d2bbeb8e0d669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 아키텍처에는 기본 Transformer 모듈만 포함되어 있으며 입력이 주어지면 feature라고도 불리는 hidden_states를 출력한다.\n",
    "\n",
    "각 모델 입력에 대해 Transformer 모델에 의해 수행된 해당 입력의 문맥적 이해 결과를 나타내는 고차원 벡처를 가져온다\n",
    "\n",
    "뭔 개소린지 모르겠는데 뒤에서 설명한다니 한 번만 참는다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dce4ea9184308ef1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 고차원 벡터?\n",
    "\n",
    "Transformer 모듈의 벡터 출력은 일반적으로 그 규모가 크다. 일반적으로 3 가지 차원이 있다.\n",
    "- Batch Size: 한 번에 처리되는 시퀀스의 개수(2)\n",
    "- Sequence Length: 시퀀스 숫자 표현의 길이(16)\n",
    "- Hidden Size: 각 모델 입력의 벡터 차원(작은 모델의 경우 768, 큰 모델의 경우 3072 이상일 수도 있음)\n",
    "\n",
    "사전 처리한 입력을 모델에 넘기면 다음과 같은 내용을 볼 수 있음."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3dc9add1bcf83a3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "wrapper.outputs = wrapper.model(**wrapper.inputs)\n",
    "print(wrapper.outputs.last_hidden_state.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:40:35.014320300Z",
     "start_time": "2023-08-10T04:40:34.986881400Z"
    }
   },
   "id": "2a52a530b357172a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "         ...,\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "         ...,\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:42:36.851855800Z",
     "start_time": "2023-08-10T04:42:36.839835900Z"
    }
   },
   "id": "8c81824a4dbc4b30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "&#129303;Transformers 모델의 출력은 `namedtuple` 또는 dictionary처럼 동작하여 요소에 접근하기 위해 속성 또는 키를 사용할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4081a8cea89c6b43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Heads: 숫자 이해하기\n",
    "\n",
    "model head는 hidden states의 고차원 벡터를 입력으로 받아 다른 차원에 투영한다.\n",
    "\n",
    "일반적으로 헤드는 하나 또는 몇 개의 선형 레이어로 구성된다.\n",
    "\n",
    "Transformer 모델의 출력은 처리할 model head로 직접 전달된다.\n",
    "\n",
    "&#129303;Transformers에는 다양한 아키텍처가 있으며 각 아키텍처는 특화된 작업을 처리하도록 설계되었다.\n",
    "- *Model (hidden states를 리턴)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- Others...&#129303;\n",
    "\n",
    "이 섹션에서의 예시에서는 시퀀스 분류 헤드가 포함되어 있는 모델이 필요하므로 AutoModel 대신 `AutoModelForSequenceClassification`을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ddd37de5d1b37dc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "wrapper.model = AutoModelForSequenceClassification.from_pretrained(wrapper.checkpoint)\n",
    "wrapper.outputs = wrapper.model(**wrapper.inputs)\n",
    "print(wrapper.outputs.logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:53:22.496042Z",
     "start_time": "2023-08-10T04:53:21.954504500Z"
    }
   },
   "id": "fd42d1808516f9cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "두 개의 레이블만 있는 모델을 통과한 2개의 모델이므로 결과의 모양은 (2, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0c808775d316a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 출력 후처리하기\n",
    "\n",
    "모델에서 출력으로 얻은 값은 그 자체로 의미있는 값은 아니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a106e6bb718e393c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.outputs.logits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T04:57:29.410831700Z",
     "start_time": "2023-08-10T04:57:29.402858900Z"
    }
   },
   "id": "79693096cb7ca54f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이는 모델의 마지막 계층에서 출력된 정규화되지 않은 원시 점수인 logits이다.\n",
    "\n",
    "이 값을 확률로 변환하려면 SoftMax 계층을 통과해야 한다.\n",
    "\n",
    "모든 &#129303;Transformers 모델은 이 logits 값을 출력하는데 그 이유는 일반적으로 학습을 위한 손실 함수는 최종 활성화 함수(activation function, e.g., SoftMax)와 실제 손실 함수(loss function, e.g., cross entropy)를 모두 사용하여 구현되기 때문이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ab5c635c0dfedc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "wrapper.predictions = torch.nn.functional.softmax(wrapper.outputs.logits, dim = -1)\n",
    "print(wrapper.predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:00:59.088054600Z",
     "start_time": "2023-08-10T05:00:59.072587400Z"
    }
   },
   "id": "213941eb73a116c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과값은 [[0.0402, 0.9598], [0.9995, 0.0005]]와 같으며 사용자가 이해할 수 있는 확률 점수이다.\n",
    "\n",
    "각 위치에 해당하는 레이블을 가져오기 위해 model.config의 `id2label` 속성값을 확인한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecbbb340a22d413a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'NEGATIVE', 1: 'POSITIVE'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.model.config.id2label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:03:20.892522300Z",
     "start_time": "2023-08-10T05:03:20.877916Z"
    }
   },
   "id": "5dad7725c87bf2ac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE: 0.0402\n",
      "POSITIVE: 0.9598\n",
      "NEGATIVE: 0.9995\n",
      "POSITIVE: 0.0005\n"
     ]
    }
   ],
   "source": [
    "for prediction in wrapper.predictions:\n",
    "    for label_id, label in wrapper.model.config.id2label.items():\n",
    "        print(f\"{label}: {prediction[label_id]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:07:09.993439800Z",
     "start_time": "2023-08-10T05:07:09.990730100Z"
    }
   },
   "id": "910930f713308217"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline과 내부에서 실행되는 3단계 비교하기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f02c904cbc22d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598050713539124}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 0.87GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "wrapper.pipeline = pipeline(\"sentiment-analysis\", wrapper.checkpoint)\n",
    "print(wrapper.pipeline(wrapper.raw_inputs))\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:16:04.289657500Z",
     "start_time": "2023-08-10T05:16:03.665300400Z"
    }
   },
   "id": "bf40bd54a71992fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Models\n",
    "\n",
    "이 섹션에서는 모델을 생성하고 사용하는 방법을 자세히 살펴본다.\n",
    "\n",
    "지정된 checkpoint를 바탕으로 모델을 인스턴스화할 때 편리한 AutoModel 클래스를 사용\n",
    "\n",
    "`AutoModel`: checkpoint에 적합한 모델 아키텍처를 자동으로 추측한 다음 이 아키텍처 모델로 인스턴스화할 수 있는 영리한 wrapper class\n",
    "\n",
    "그러나 사용하려는 모델의 유형을 알고있다면 해당 아키텍처를 직접 정의하는 클래스를 사용할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f78b00e3ad43761e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer Model 생성하기\n",
    "\n",
    "BERT 모델과 함께 어떻게 작동하는지 살펴본다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed467e0a487d3b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "wrapper.config = BertConfig()\n",
    "\n",
    "wrapper.model = BertModel(wrapper.config) # 또는 BertModel.from_pretrained(...)\n",
    "\n",
    "print(wrapper.config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:16:43.999106600Z",
     "start_time": "2023-08-10T05:16:43.363073Z"
    }
   },
   "id": "d79d30c80b000bbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Method\n",
    "\n",
    "`save_pretrained()`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4222076257d9b728"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "wrapper.model.save_pretrained(\"../models/02.models\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:21:32.285360Z",
     "start_time": "2023-08-10T05:21:32.029672200Z"
    }
   },
   "id": "13c409586611498d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- config.json: model architecture와 다양한 config 속성들, 모델 아키텍처 파악용\n",
    "- pytorch_model.bin: state dictionary, 모델의 모든 가중치가 저장되어있음, 모델의 파라미터"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2b5ed308f0f5a3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer model을 활용한 추론(inference)\n",
    "\n",
    "Transformer 모델은 토크나이저가 생성하는 숫자만 처리할 수 있다.\n",
    "\n",
    "모델이 허용하는 입력은 행렬 형태의 2중 리스트만 허용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc8dbf3eb38ef2d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592,  999,  102],\n",
      "        [ 101, 4658, 1012,  102],\n",
      "        [ 101, 3835,  999,  102]])\n"
     ]
    }
   ],
   "source": [
    "wrapper.encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102]\n",
    "]\n",
    "\n",
    "wrapper.model_inputs = torch.tensor(wrapper.encoded_sequences)\n",
    "\n",
    "print(wrapper.model_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:28:15.695522900Z",
     "start_time": "2023-08-10T05:28:15.680623100Z"
    }
   },
   "id": "92d906975e4ae43a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model의 입력으로 텐서 활용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa3b2e4c60178068"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1437, -1.8536, -0.7298,  ..., -0.7805,  0.3259,  0.4658],\n",
      "         [ 0.3714, -0.9110, -0.8022,  ...,  0.3886,  0.8540,  0.6429],\n",
      "         [-0.4563, -0.4854, -0.9873,  ...,  0.1407,  0.5035,  0.1391],\n",
      "         [ 0.2318, -0.1520, -0.9008,  ...,  0.3321,  0.7286,  0.4390]],\n",
      "\n",
      "        [[-1.9583, -1.9658, -0.4219,  ..., -0.8799,  0.8664,  0.7912],\n",
      "         [-0.5089,  0.4376,  0.2652,  ..., -0.2958,  0.8589,  0.5303],\n",
      "         [-0.3855, -0.0138, -0.4196,  ...,  0.0687,  0.5166, -0.3751],\n",
      "         [-1.2665, -0.6342, -0.3569,  ...,  0.0830, -0.2257,  0.6619]],\n",
      "\n",
      "        [[-0.4953, -1.8715, -0.8113,  ..., -0.5110,  1.4664,  1.1589],\n",
      "         [ 0.9335, -0.7509, -0.5613,  ...,  0.4530,  1.5268, -0.5890],\n",
      "         [-0.4374, -0.6369, -1.3684,  ..., -0.7098,  0.9980, -0.0311],\n",
      "         [-0.1259,  0.3466, -0.7542,  ..., -0.7339,  0.2409,  1.0125]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.5303,  0.0610, -0.3573,  ..., -0.3626, -0.4339,  0.2605],\n",
      "        [ 0.3617, -0.0212, -0.4189,  ..., -0.0763,  0.1826,  0.4405],\n",
      "        [ 0.7082,  0.0480, -0.2155,  ..., -0.2743, -0.5024,  0.0648]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "wrapper.output = wrapper.model(wrapper.model_inputs)\n",
    "print(wrapper.output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:28:55.084347200Z",
     "start_time": "2023-08-10T05:28:55.051063100Z"
    }
   },
   "id": "50cf70015efb5322"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Tokenizer\n",
    "\n",
    "NLP의 핵심 구형 요소 중 하나. vocabulary에 없는 단어를 표현하기 위해 Unknown token [UNK]를 사용하는데 이를 줄이는 한 가지 방법은 문자기반 토크나이저를 사용하는 것이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4efaba49e904ea48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Character-based Tokenization\n",
    "\n",
    "장점\n",
    "- vocabulary의 크기가 매우 작음\n",
    "- OOV(Out Of Vocabulary) 토큰이 훨씬 적음\n",
    "\n",
    "단점\n",
    "- 토큰 자체가 직관적이지 않음\n",
    "- 토큰의 양이 너무 많음\n",
    "\n",
    "단어기반 + 문자기반 => 하위단어 토큰화가 탄생함"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a4e9af80e95d16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subword Tokenization\n",
    "\n",
    "빈번하게 사용하는 단어는 더 작은 하위단어로 분할하지 않고 희귀 단어를 의미있는 하위 단어로 분할한다는 원칙에 기반한다.\n",
    "\n",
    "subwords를 연결하여 길이가 긴 복잡한 단어를 임의로 만들 수 있는 터키어와 같은 교착 언어에서 특히 유용함. ~~한국어도 교착어인디..?~~\n",
    "\n",
    "#### 세부 기법들\n",
    "- BBPE(Byte-level Byte Pair Encoding): GPT-2에 사용됨\n",
    "- WordPiece: BERT에 사용됨\n",
    "- SentencePiece, Unigram: 몇몇 다국어 모델에 사용됨"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "657b55562d1d53d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer 로딩 및 저장\n",
    "\n",
    "`save_pretrained()`, `from_pretrained()`\n",
    ": Model과 같은 방법으로 로딩 및 저장할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d597f2953e82c501"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 0.81GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "wrapper.auto_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "wrapper.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def is_same(sentenceA, sentenceB) -> bool:\n",
    "    items_a = sentenceA.items()\n",
    "    items_b = sentenceB.items()\n",
    "    \n",
    "    sorted(items_a, key = lambda x: x[0])\n",
    "    sorted(items_b, key = lambda x: x[0])\n",
    "    \n",
    "    for item_a, item_b in zip(items_a, items_b):\n",
    "        if item_a[0] != item_b[0]:\n",
    "            return False\n",
    "        if len(item_a[1]) != len(item_b[1]):\n",
    "            return False\n",
    "        for item_a1, item_b1 in zip(item_a[1], item_b[1]):\n",
    "            if item_a1 != item_b1:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "wrapper.inputs = \"Using a Transformer network is simple\"\n",
    "print(\"Same\") if is_same(wrapper.auto_tokenizer(wrapper.inputs), wrapper.bert_tokenizer(wrapper.inputs)) else print(\"Different\")\n",
    "\n",
    "wrapper.auto_tokenizer.save_pretrained(\"../models/02.models\")\n",
    "\n",
    "del is_same\n",
    "wrapper.init_wrapper()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T05:59:25.243332Z",
     "start_time": "2023-08-10T05:59:24.682013600Z"
    }
   },
   "id": "903c689a54a6fcc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding\n",
    "\n",
    "Translating Text to Numbers Process\n",
    "1. Tokenize\n",
    "2. From Tokens To Input IDs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a524c048b79ac8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenize\n",
    "\n",
    "토큰화 프로세스는 Tokenizer의 `tokenize()`에 이해 수행됨."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4523f7fd7ccd2c0b"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "wrapper.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "wrapper.sequence = \"Using a Transformer network is simple\"\n",
    "wrapper.tokens = wrapper.tokenizer.tokenize(wrapper.sequence)\n",
    "\n",
    "print(wrapper.tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T06:00:44.394433500Z",
     "start_time": "2023-08-10T06:00:44.190146200Z"
    }
   },
   "id": "c2a691acb82b29b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### From Tokens To Input IDs\n",
    "\n",
    "`convert_tokens_to_ids()` 메서드에 의해 처리"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d9b8455439a1176"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "wrapper.ids = wrapper.tokenizer.convert_tokens_to_ids(wrapper.tokens)\n",
    "print(wrapper.ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T06:01:58.439310900Z",
     "start_time": "2023-08-10T06:01:58.419290700Z"
    }
   },
   "id": "bc3076030843d2b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoding\n",
    "\n",
    "Encoding의 반대 방향으로 진행되지만 `decode()`로 한 번에 해결"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef0683899281aad"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Transformer network is simple\n",
      "\n",
      "GPU NVIDIA GeForce RTX 3060\n",
      "memory occupied: 0.80GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "wrapper.decoded_string = wrapper.tokenizer.decode(wrapper.ids)\n",
    "print(wrapper.decoded_string)\n",
    "\n",
    "wrapper.init_wrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T06:03:30.510181300Z",
     "start_time": "2023-08-10T06:03:30.445108200Z"
    }
   },
   "id": "25476933fc92efff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 다중 시퀀스 처리\n",
    "\n",
    "## model은 입력의 batch 형태를 요구한다.\n",
    "\n",
    "model의 `_call_impl(self, *inputs, **kwargs)` 메서드를 보면 input string이 리스트 형태로 `*inputs`에 들어가있다.\n",
    "\n",
    "input string이 한 개의 문자열 텐서(단일 리스트)라면 에러가 발생한다.\n",
    "\n",
    "Tokenizer의 `__call__(...)` return 값이 2중 리스트인 것도 그런 이유 때문\n",
    "\n",
    "따라서 단일 문자열을 텐서로 변환한 후 model에 전달하려고 할 때 텐서는 이렇게 만들어야 한다.\n",
    "```python\n",
    "import torch\n",
    "\n",
    "ids = [] # tokens\n",
    "input_ids = torch.tensor([ids])\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2d622ba34f18c5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding / Truncation\n",
    "\n",
    "문자열을 batch로 전달할 때 모든 문자열이 길이가 같을 수는 없다.\n",
    "\n",
    "따라서 짧은 문자열에 padding을 추가해 attention_mask를 추가하거나 긴 문자열은 truncate하여 정사각형 텐서를 만들어야 한다.\n",
    "\n",
    "방법은 Tokenizer 적용 시 두 개의 parameter를 추가한다.\n",
    "- padding = True\n",
    "- truncation = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee5425721dd604d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6686bc6a135a6e43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
